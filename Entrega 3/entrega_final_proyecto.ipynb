{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e6ac90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ENTORNO LISTO\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from unidecode import unidecode\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Rutas\n",
    "BASE_DIR = os.getcwd()\n",
    "INPUT_FILE = os.path.join(BASE_DIR, \"2053806800_Aseguradoras+Query.xlsx\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"Resultados_Fase3\")\n",
    "if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Descargas necesarias\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\">>> ENTORNO LISTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40d8cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Cargando datos...\n",
      "   - Generando versiones de texto...\n",
      "âœ… Preprocesamiento listo: 15469 registros.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  CELDA 2: CARGA Y PREPROCESAMIENTO (DOS VERSIONES DE TEXTO)\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Definiciones de Regex\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", flags=re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r\"@\\w+\", flags=re.IGNORECASE)\n",
    "HASHTAG_RE = re.compile(r\"#\\w+\", flags=re.IGNORECASE)\n",
    "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "# 2. Funciones de Limpieza\n",
    "def basic_clean(text: str) -> str:\n",
    "    \"\"\"Limpieza SUAVE: Mantiene tildes (Para Sentimiento).\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    t = text\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = MENTION_RE.sub(lambda m: \" \" + m.group(0)[1:] + \" \", t)\n",
    "    t = HASHTAG_RE.sub(lambda m: \" \" + m.group(0)[1:] + \" \", t)\n",
    "    t = t.replace(\"â€™\", \"'\").replace(\"â€œ\", '\"').replace(\"â€\", '\"')\n",
    "    t = t.lower() # Solo minÃºsculas\n",
    "    t = MULTISPACE_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def ascii_fold(text: str) -> str:\n",
    "    \"\"\"Limpieza FUERTE: Quita tildes (Para K-Means/Topics).\"\"\"\n",
    "    return unidecode(text or \"\").lower()\n",
    "\n",
    "# 3. Carga y AplicaciÃ³n\n",
    "print(\"âš™ï¸ Cargando datos...\")\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "if \"id\" not in df.columns: df.insert(0, \"id\", range(1, len(df)+1))\n",
    "\n",
    "# ConstrucciÃ³n del texto crudo\n",
    "df[\"text_raw\"] = (df.get(\"Title\", \"\").fillna(\"\").astype(str) + \" \" + \n",
    "                  df.get(\"Snippet\", \"\").fillna(\"\").astype(str)).str.strip()\n",
    "\n",
    "# AplicaciÃ³n de limpiezas\n",
    "print(\"   - Generando versiones de texto...\")\n",
    "df[\"text_clean\"] = df[\"text_raw\"].map(basic_clean)       # Con tildes\n",
    "df[\"text_norm\"] = df[\"text_clean\"].map(ascii_fold)       # Sin tildes\n",
    "\n",
    "# 4. ExtracciÃ³n de Marcas y Exclusiones\n",
    "BRANDS = {\n",
    "    \"seguros bolivar\": {\"seguros bolÃ­var\", \"seguros bolivar\", \"@segurosbolivar\", \"grupobolivar\"},\n",
    "    \"sura\": {\"sura\", \"grupo sura\", \"@suramericana\", \"suramericana\", \"gruposura\", \"eps sura\"},\n",
    "    \"axa colpatria\": {\"axa colpatria\", \"@axacolpatria\", \"colpatria seguros\",\"axacolpatria\"},\n",
    "    \"mapfre\": {\"mapfre\", \"@mapfre_col\",\"mapfre_co\", \"fundacion mapfre\"},\n",
    "    \"allianz\": {\"allianz\", \"@allianzcol\", \"allianz colombia\"},\n",
    "    \"liberty seguros\": {\"liberty seguros\", \"@libertycol\"},\n",
    "    \"zurich\": {\"zurich\", \"@zurichcolombia\"},\n",
    "    \"hdi seguros\": {\"hdi seguros\", \"@hdi_segurosco\"},\n",
    "    \"seguros del estado\": {\"seguros del estado\", \"@segurosdelestado\"},\n",
    "    \"porvenir\": {\"porvenir\", \"@porvenir\"},\n",
    "    \"positiva\": {\"positiva compaÃ±Ã­a de seguros\", \"positivacol\", \"positiva\"},\n",
    "    \"previsora\": {\"fiduciaria la previsora\", \"previsora\", \"previsora s.a.\"}\n",
    "}\n",
    "\n",
    "def extract_brand(text):\n",
    "    t = \" \" + text + \" \"\n",
    "    for brand, aliases in BRANDS.items():\n",
    "        for alias in aliases | {brand}:\n",
    "            if f\" {ascii_fold(alias)} \" in t: return brand\n",
    "    return \"Otras\"\n",
    "\n",
    "CORP_EXCLUDE = {\"grupo_sura\",\"grupo argos\",\"junta directiv\",\"accionistas\",\"utilidad neta\",\"colcap\"}\n",
    "CM_EXCLUDE = {\"mensaje directo\",\"mensaje privado\",\"favor escrib\",\"invitamos contactar\"}\n",
    "\n",
    "def is_excluded(text):\n",
    "    t = ascii_fold(text)\n",
    "    return any(p in t for p in CORP_EXCLUDE) or any(p in t for p in CM_EXCLUDE)\n",
    "\n",
    "df[\"brand_primary\"] = df[\"text_norm\"].map(extract_brand)\n",
    "df[\"flag_exclude\"] = df[\"text_norm\"].map(is_excluded)\n",
    "\n",
    "print(f\"âœ… Preprocesamiento listo: {len(df)} registros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02d2ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Calculando Sentimiento (LÃ³gica Original)...\n",
      "   - Descargando desde https://raw.githubusercontent.com/ITALIC-US/ML-Senticon/main/senticon.es.xml...\n",
      "   - LexicÃ³n guardado.\n",
      "   - Entradas crudas encontradas: 11542\n",
      "âœ… Diccionario Final: 11342 tÃ©rminos cargados.\n",
      "\n",
      ">>> DISTRIBUCIÃ“N RECUPERADA <<<\n",
      "lex_label\n",
      "pos    8071\n",
      "neu    4567\n",
      "neg    2831\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  CELDA 3: SENTIMIENTO (RESTAURACIÃ“N FIDEDIGNA DEL CÃ“DIGO ORIGINAL)\n",
    "# ==============================================================================\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "print(\"âš™ï¸ Calculando Sentimiento (LÃ³gica Original)...\")\n",
    "\n",
    "# 1. Descarga (IdÃ©ntico a tu cÃ³digo)\n",
    "LEX_XML = os.path.join(BASE_DIR, \"mlsenticon_es.xml\")\n",
    "# Borramos si existe para asegurar una descarga limpia si fallÃ³ antes\n",
    "if os.path.exists(LEX_XML):\n",
    "    try: os.remove(LEX_XML)\n",
    "    except: pass\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ITALIC-US/ML-Senticon/main/senticon.es.xml\"\n",
    "print(f\"   - Descargando desde {url}...\")\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(LEX_XML, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"   - LexicÃ³n guardado.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error descarga: {e}\")\n",
    "\n",
    "# 2. Parsear XML a DataFrame (TU LÃ“GICA EXACTA)\n",
    "rows = []\n",
    "try:\n",
    "    tree = ET.parse(LEX_XML)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    POL_KEYS = {\"polarity\", \"priorpolarity\", \"sentiment\", \"value\", \"pol\", \"score\", \"strength\"}\n",
    "\n",
    "    def extract_polarity_from_attrib(d):\n",
    "        for k, v in d.items():\n",
    "            if k.lower() in POL_KEYS and v:\n",
    "                return str(v).strip()\n",
    "        return None\n",
    "\n",
    "    def norm_tag(t): # Helper para evitar problemas de namespace en XML\n",
    "        return t.split('}')[-1].lower() if '}' in t else t.lower()\n",
    "\n",
    "    for lemma in root.iter():\n",
    "        if \"lemma\" not in norm_tag(lemma.tag):\n",
    "            continue\n",
    "        \n",
    "        # Obtener la palabra\n",
    "        form = lemma.attrib.get(\"form\") or lemma.attrib.get(\"word\") or (lemma.text or \"\").strip()\n",
    "        if not form:\n",
    "            continue\n",
    "        form = form.lower().strip()\n",
    "\n",
    "        # 1) Caso A: atributo directo\n",
    "        pol = extract_polarity_from_attrib(lemma.attrib)\n",
    "\n",
    "        # 2) Caso B: buscar en hijos\n",
    "        if pol is None:\n",
    "            for child in lemma:\n",
    "                pol = extract_polarity_from_attrib(child.attrib)\n",
    "                if pol:\n",
    "                    break\n",
    "        \n",
    "        if pol:\n",
    "            rows.append((form, pol))\n",
    "\n",
    "    print(f\"   - Entradas crudas encontradas: {len(rows)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error parseando XML: {e}\")\n",
    "\n",
    "# 3. Normalizar y Crear Diccionario\n",
    "if rows:\n",
    "    lex_df = pd.DataFrame(rows, columns=[\"lemma\", \"polarity\"])\n",
    "    \n",
    "    def _map_polarity(x):\n",
    "        x = str(x).strip().lower()\n",
    "        if x in (\"positive\",\"pos\",\"+\",\"1\",\"true\"): return 1.0\n",
    "        if x in (\"negative\",\"neg\",\"-\",\"-1\",\"false\"): return -1.0\n",
    "        if x in (\"neutral\",\"neu\",\"0\"): return 0.0\n",
    "        try: return float(x)\n",
    "        except: return 0.0\n",
    "\n",
    "    lex_df[\"pol\"] = lex_df[\"polarity\"].map(_map_polarity)\n",
    "    # Promediar duplicados (LÃ³gica original)\n",
    "    lex_df = lex_df.groupby(\"lemma\", as_index=False)[\"pol\"].mean()\n",
    "    lexicon = dict(zip(lex_df[\"lemma\"], lex_df[\"pol\"]))\n",
    "    \n",
    "    print(f\"âœ… Diccionario Final: {len(lexicon)} tÃ©rminos cargados.\")\n",
    "else:\n",
    "    print(\"âŒ EL DICCIONARIO ESTÃ VACÃO. Revisa la descarga.\")\n",
    "    lexicon = {}\n",
    "\n",
    "# 4. Ajustes de Dominio (Tus ajustes originales)\n",
    "lexicon.update({\n",
    "    \"queja\": -0.6, \"reclamo\": -0.6, \"radicado\": -0.2,\n",
    "    \"siniestro\": -0.1, \"indemnizacion\": 0.2, \"indemnizaciÃ³n\": 0.2,\n",
    "    \"pqrs\": -0.4, \"soat\": 0.0, \"fraude\": -0.5, \"atencion\": -0.2\n",
    "})\n",
    "\n",
    "# 5. CÃ¡lculo (Tu funciÃ³n original con While)\n",
    "TOKEN_RE = re.compile(r\"[A-Za-zÃÃ‰ÃÃ“ÃšÃœÃ‘Ã¡Ã©Ã­Ã³ÃºÃ¼Ã±0-9_]+\", re.UNICODE)\n",
    "NEGATORS = {\"no\",\"nunca\",\"jamÃ¡s\",\"jamas\",\"ninguno\",\"ninguna\",\"nadie\",\"tampoco\",\"sin\"}\n",
    "INTENSIFIERS = {\"muy\":1.5,\"super\":1.5,\"sÃºper\":1.5,\"tan\":1.2,\"bastante\":1.3,\"demasiado\":1.3}\n",
    "NEG_WINDOW = 3\n",
    "\n",
    "def sentiment_score(text):\n",
    "    if not isinstance(text, str): return 0.0\n",
    "    # Usamos text_clean que TIENE TILDES (Generado en Celda 2)\n",
    "    tokens = TOKEN_RE.findall(text.lower())\n",
    "    score = 0.0\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        tok = tokens[i]\n",
    "        mult = INTENSIFIERS.get(tok, 1.0)\n",
    "        if tok in NEGATORS:\n",
    "            j = i + 1\n",
    "            flips = 0\n",
    "            while j < len(tokens) and flips < NEG_WINDOW:\n",
    "                pol = lexicon.get(tokens[j], 0.0)\n",
    "                if pol != 0.0:\n",
    "                    score += -pol\n",
    "                    flips += 1\n",
    "                j += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        pol = lexicon.get(tok, 0.0)\n",
    "        if pol != 0.0:\n",
    "            score += pol * mult\n",
    "        i += 1\n",
    "    return score\n",
    "\n",
    "# AplicaciÃ³n\n",
    "df[\"sent_score\"] = df[\"text_clean\"].map(sentiment_score)\n",
    "\n",
    "def get_label(s):\n",
    "    if s >= 0.25: return \"pos\"\n",
    "    if s <= -0.25: return \"neg\"\n",
    "    return \"neu\"\n",
    "\n",
    "df[\"lex_label\"] = df[\"sent_score\"].map(get_label)\n",
    "print(\"\\n>>> DISTRIBUCIÃ“N RECUPERADA <<<\")\n",
    "print(df[\"lex_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a675f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Ejecutando K-Means...\n",
      "âœ… Clustering listo: 12390 registros clasificados.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  CELDA 4: K-MEANS CLUSTERING\n",
    "# ==============================================================================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"âš™ï¸ Ejecutando K-Means...\")\n",
    "\n",
    "# Stopwords\n",
    "stop_es = set(stopwords.words('spanish'))\n",
    "DOMAIN_STOPWORDS = {\n",
    "    \"seguros\", \"aseguradora\", \"aseguradoras\", \"pÃ³liza\", \"poliza\", \"asegurado\", \"asegurada\",\n",
    "    \"cliente\", \"clientes\", \"grupo\", \"fundacion\", \"colombia\", \"colombiano\", \"empresa\", \n",
    "    \"https\", \"http\", \"co\", \"com\", \"www\", \"status\", \"web\", \"twitter\", \"x\", \"tweet\",\n",
    "    *set(BRANDS.keys())\n",
    "}\n",
    "FULL_STOPWORDS = list(stop_es | DOMAIN_STOPWORDS)\n",
    "\n",
    "# Filtro modelo\n",
    "df_model = df[~df[\"flag_exclude\"]].copy()\n",
    "df_model = df_model[df_model[\"text_norm\"].str.len() > 10]\n",
    "\n",
    "# VectorizaciÃ³n (Sobre text_norm para estandarizar)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=10, max_df=0.5, max_features=8000, stop_words=FULL_STOPWORDS)\n",
    "X = tfidf.fit_transform(df_model[\"text_norm\"].fillna(\"\"))\n",
    "\n",
    "# K-Means\n",
    "K = 7\n",
    "km = MiniBatchKMeans(n_clusters=K, random_state=42, batch_size=2048, n_init=\"auto\")\n",
    "df_model[\"topic_id\"] = km.fit_predict(X)\n",
    "\n",
    "print(f\"âœ… Clustering listo: {len(df_model)} registros clasificados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23c38ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Consultando a Gemini...\n",
      "ðŸ¤– Mapeo generado: {0: 'Resultados Financieros Ecopetrol/EPM', 1: 'Sistema Salud y EPS', 2: 'Ubicaciones GeogrÃ¡ficas', 3: 'Cambios en Liderazgo Asegurador', 4: 'Seguro SOAT y GestiÃ³n de Riesgos', 5: 'Resultados Empresas Bolsa', 6: 'Servicios Aseguradores Digitales'}\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  CELDA 5: AUTO-ETIQUETADO GEMINI\n",
    "# ==============================================================================\n",
    "import google.generativeai as genai\n",
    "import ast\n",
    "\n",
    "print(\"âš™ï¸ Consultando a Gemini...\")\n",
    "\n",
    "# API KEY\n",
    "API_KEY = \"AIzaSyA04JNJ6-uxgTH76kxnbXIP0E0GM8-pHBU\" \n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# Usamos flash por velocidad y eficiencia\n",
    "model = genai.GenerativeModel('gemini-2.5-flash-lite') \n",
    "\n",
    "# Extraer palabras\n",
    "terms = tfidf.get_feature_names_out()\n",
    "centroids = km.cluster_centers_\n",
    "info = \"\"\n",
    "for i in range(K):\n",
    "    top = [terms[ind] for ind in centroids[i].argsort()[::-1][:15]]\n",
    "    info += f\"Cluster {i}: {', '.join(top)}\\n\"\n",
    "\n",
    "# Prompt\n",
    "prompt = f\"\"\"\n",
    "ActÃºa como experto en seguros Colombia. Analiza estos clusters de tweets:\n",
    "{info}\n",
    "Genera un diccionario Python {{id: \"Nombre Tema\"}} con nombres cortos de negocio.\n",
    "Reglas:\n",
    "- 'olmedo/ungrd' -> \"EscÃ¡ndalo CorrupciÃ³n\"\n",
    "- 'vacante/trabajo' -> \"Ofertas Empleo\"\n",
    "- 'patinaje' -> \"Patrocinios\"\n",
    "- 'bolivar/davivienda' -> \"Corporativo\"\n",
    "RESPONDER SOLO EL DICCIONARIO.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    resp = model.generate_content(prompt)\n",
    "    txt = resp.text.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    MAPEO = ast.literal_eval(txt)\n",
    "    print(\"ðŸ¤– Mapeo generado:\", MAPEO)\n",
    "except:\n",
    "    print(\"âŒ FallÃ³ Gemini, usando genÃ©rico.\")\n",
    "    MAPEO = {i: f\"Tema {i}\" for i in range(K)}\n",
    "\n",
    "df_model[\"topic_name\"] = df_model[\"topic_id\"].map(MAPEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a991cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Generando Dataset Maestro...\n",
      "\n",
      ">>> DATOS FINALES <<<\n",
      "sentimiento\n",
      "Positivo    8071\n",
      "Neutro      4567\n",
      "Negativo    2831\n",
      "Name: count, dtype: int64\n",
      "topic_name\n",
      "Servicios Aseguradores Digitales    6488\n",
      "Excluido/Ruido                      3079\n",
      "Sistema Salud y EPS                 2242\n",
      "Resultados Empresas Bolsa           1832\n",
      "Cambios en Liderazgo Asegurador      967\n",
      "Name: count, dtype: int64\n",
      "âœ… Archivo guardado: c:\\Users\\santi\\Downloads\\Learning\\Maestria\\Programacion Lenguaje Natural\\Proyecto\\Entrega 3\\Resultados_Fase3\\DATASET_FINAL_MAESTRO.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  CELDA 6: INTEGRACIÃ“N FINAL Y EXPORTACIÃ“N\n",
    "# ==============================================================================\n",
    "print(\"âš™ï¸ Generando Dataset Maestro...\")\n",
    "\n",
    "# 1. Mapeo Sentimiento para App\n",
    "mapa_app = {\"pos\": \"Positivo\", \"neg\": \"Negativo\", \"neu\": \"Neutro\"}\n",
    "df[\"sentimiento\"] = df[\"lex_label\"].map(mapa_app).fillna(\"Neutro\")\n",
    "\n",
    "# 2. Integrar Temas\n",
    "# Limpiar previos\n",
    "if \"topic_name\" in df.columns: df = df.drop(columns=[\"topic_name\"])\n",
    "\n",
    "# Merge\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "df_model[\"id\"] = df_model[\"id\"].astype(int)\n",
    "df = df.merge(df_model[[\"id\", \"topic_name\"]], on=\"id\", how=\"left\")\n",
    "df[\"topic_name\"] = df[\"topic_name\"].fillna(\"Excluido/Ruido\")\n",
    "\n",
    "# 3. ValidaciÃ³n Final\n",
    "print(\"\\n>>> DATOS FINALES <<<\")\n",
    "print(df[\"sentimiento\"].value_counts())\n",
    "print(df[\"topic_name\"].value_counts().head())\n",
    "\n",
    "# 4. Guardar\n",
    "outfile = os.path.join(OUTPUT_DIR, \"DATASET_FINAL_MAESTRO.csv\")\n",
    "df.to_csv(outfile, index=False, sep=\";\", encoding=\"utf-8\")\n",
    "print(f\"âœ… Archivo guardado: {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbdc44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
