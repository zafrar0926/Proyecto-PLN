{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151636aa",
   "metadata": {},
   "source": [
    "# EXPERIMENTACIÓN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec96a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros cargados: 15469\n",
      "                              n_docs   mean_len  mean_ttr\n",
      "brand_primary                                            \n",
      "                                4987  53.885101  0.627808\n",
      "seguros bolivar                 3379  56.077242  0.557132\n",
      "Fiduciaria La Previsora         1915  59.916971  0.582862\n",
      "sura                            1702  56.796122  0.631199\n",
      "mapfre                          1187  55.144061  0.636045\n",
      "Positiva Compañía de Seguros     933  58.722401  0.452000\n",
      "axa colpatria                    663  56.457014  0.578748\n",
      "allianz                          316  53.050633  0.686732\n",
      "liberty seguros                  186  54.258065  0.690902\n",
      "hdi seguros                      109  59.155963  0.659051\n",
      "Exportado: corpus_medido2.csv (y parquet si estuvo disponible)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  PLN - Proyecto Tweets Aseguradoras (Módulo 1: Léxico)\n",
    "#  Versión ajustada para mejorar detección de marcas\n",
    "# ============================================================\n",
    "\n",
    "# --------- 0) Setup de entorno (instala si falta) ----------\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name or pkg])\n",
    "\n",
    "# Librerías base\n",
    "ensure(\"pandas\")\n",
    "ensure(\"numpy\", \"numpy<2.0\")\n",
    "ensure(\"regex\")\n",
    "ensure(\"unidecode\")\n",
    "ensure(\"nltk\")\n",
    "ensure(\"sklearn\", \"scikit-learn\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "\n",
    "# spaCy (modelo español)\n",
    "ensure(\"spacy\")\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\"])\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# --------- 1) Parámetros de proyecto -----------------------\n",
    "EXCEL_PATH = r\"C:\\Users\\santi\\Downloads\\Learning\\Maestria\\Programacion Lenguaje Natural\\Proyecto\\Inputs\\2053806800_Aseguradoras+Query.xlsx\"\n",
    "SHEET_NAME = \"Hoja1\"\n",
    "\n",
    "# Columnas esperadas (ajusta si varían)\n",
    "COL_DATE   = \"Date\"\n",
    "COL_TITLE  = \"Title\"\n",
    "COL_SNIPP  = \"Snippet\"\n",
    "COL_URL    = \"Url\"\n",
    "COL_DOMAIN = \"Domain\"\n",
    "COL_LANG   = \"Language\"\n",
    "COL_COUNTRY = \"Country\"\n",
    "TEXT_COL   = \"text_raw\"\n",
    "\n",
    "# MWE (frases fijas)\n",
    "MWE_SERVICE = [\n",
    "    \"atención al cliente\", \"pago de siniestro\", \"tiempo de respuesta\",\n",
    "    \"radicado de pqrs\", \"red médica\", \"autorización de servicio\",\n",
    "    \"incremento de prima\", \"línea de atención\"\n",
    "]\n",
    "\n",
    "MWE_ENTITIES = [\n",
    "    \"grupo sura\", \"fondo de pensiones obligatorias\"\n",
    "]\n",
    "\n",
    "# Gazetteer de marcas\n",
    "BRANDS = {\n",
    "    \"seguros bolivar\": {\"seguros bolívar\", \"seguros bolivar\", \"@segurosbolivar\",\n",
    "                        \"grupobolivar\", \"grupo bolívar\", \"grupo bolivar\", \"segurosbolivar\"},\n",
    "    \"sura\": {\"sura\", \"grupo sura\", \"@suramericana\", \"suramericana\"},\n",
    "    \"axa colpatria\": {\"axa colpatria\", \"@axacolpatria\", \"colpatria seguros\",\"axacolpatria\"},\n",
    "    \"mapfre\": {\"mapfre\", \"@mapfre_col\",\"mapfre_co\"},\n",
    "    \"allianz\": {\"allianz\", \"@allianzcol\"},\n",
    "    \"liberty seguros\": {\"liberty seguros\", \"@libertycol\"},\n",
    "    \"zurich\": {\"zurich\", \"@zurichcolombia\"},\n",
    "    \"hdi seguros\": {\"hdi seguros\", \"@hdi_segurosco\"},\n",
    "    \"seguros del estado\": {\"seguros del estado\", \"@segurosdelestado\"},\n",
    "    \"porvenir\": {\"porvenir\", \"@porvenir\", \"fondo de pensiones obligatorias porvenir\"},\n",
    "    \"Fiduciaria La Previsora\": {\"fiduciaria la previsora\", \"@fiduciarialaprevisora\",\n",
    "                                \"fiduciaria la previsora\",\"previsora\",\"la previsora s.a. compañía de seguros\"},\n",
    "    \"Positiva Compañía de Seguros\": {\"positiva compañía de seguros\", \"positivacol\"},\n",
    "}\n",
    "\n",
    "# Mapping dominio -> marca\n",
    "DOMAIN2BRAND = {\n",
    "    \"segurosbolivar.com\": \"seguros bolivar\",\n",
    "    \"grupobolivar.com\": \"seguros bolivar\",\n",
    "    \"sura.com\": \"sura\",\n",
    "    \"segurossura.com\": \"sura\",\n",
    "    \"axacolpatria.co\": \"axa colpatria\",\n",
    "    \"mapfre.com\": \"mapfre\",\n",
    "    \"allianz.co\": \"allianz\",\n",
    "    \"libertycolombia.com\": \"liberty seguros\",\n",
    "    \"zurich.com.co\": \"zurich\",\n",
    "    \"hdi.com.co\": \"hdi seguros\",\n",
    "    \"segurosdelestado.com\": \"seguros del estado\",\n",
    "    \"porvenir.com.co\": \"porvenir\",\n",
    "}\n",
    "\n",
    "# Stopwords de dominio\n",
    "DOMAIN_STOPWORDS = {\n",
    "    \"seguros\", \"aseguradora\", \"aseguradoras\", \"póliza\", \"poliza\", \"asegurado\", \"asegurada\",\n",
    "    *set(DOMAIN2BRAND.values()), *set(BRANDS.keys()),\n",
    "}\n",
    "\n",
    "# --------- 2) Normalización ------------------\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", flags=re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r\"@\\w+\", flags=re.IGNORECASE)\n",
    "HASHTAG_RE = re.compile(r\"#\\w+\", flags=re.IGNORECASE)\n",
    "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    \"\"\"Limpieza robusta: conserva menciones y hashtags como texto legible.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text\n",
    "\n",
    "    # ⚙️ Guardar dominios antes de eliminar URLs\n",
    "    urls = re.findall(URL_RE, t)\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "\n",
    "    # ⚙️ Mantener menciones sin @ (para que 'segurosbolivar' siga apareciendo)\n",
    "    t = MENTION_RE.sub(lambda m: \" \" + m.group(0)[1:] + \" \", t)\n",
    "\n",
    "    # ⚙️ Conservar hashtags como palabras (sin #)\n",
    "    t = HASHTAG_RE.sub(lambda m: \" \" + m.group(0)[1:] + \" \", t)\n",
    "\n",
    "    # Limpieza general\n",
    "    t = t.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    t = t.lower()\n",
    "    t = MULTISPACE_RE.sub(\" \", t).strip()\n",
    "\n",
    "    # ⚙️ Añadir URLs al final para detección de dominio\n",
    "    if urls:\n",
    "        t += \" \" + \" \".join(urls)\n",
    "    return t\n",
    "\n",
    "\n",
    "def ascii_fold(text: str) -> str:\n",
    "    \"\"\"Quita tildes y normaliza a minúsculas.\"\"\"\n",
    "    return unidecode(text or \"\").lower()\n",
    "\n",
    "def block_mwes(text: str, mwe_list) -> str:\n",
    "    \"\"\"Reemplaza espacios en MWE por '_' para tratarlas como tokens únicos.\"\"\"\n",
    "    t = \" \" + text + \" \"\n",
    "    for phrase in sorted(mwe_list, key=len, reverse=True):\n",
    "        p_norm = ascii_fold(phrase)\n",
    "        pattern = re.escape(p_norm).replace(r\"\\ \", r\"\\s+\")\n",
    "        rx = re.compile(rf\"(?i)(?<!\\w){pattern}(?!\\w)\")\n",
    "        t_norm = ascii_fold(t)\n",
    "        t_norm = rx.sub(phrase.replace(\" \", \"_\"), t_norm)\n",
    "        t = t_norm\n",
    "    return t.strip()\n",
    "\n",
    "# --------- 3) Extracción de marcas -------------------------\n",
    "def extract_brands(text_norm: str, domain: str = \"\") -> list:\n",
    "    \"\"\"Detección flexible de marcas: menciones pegadas, hashtags y dominios.\"\"\"\n",
    "    found = set()\n",
    "    t = ascii_fold(text_norm)\n",
    "\n",
    "    # Búsqueda por texto\n",
    "    for brand, aliases in BRANDS.items():\n",
    "        for alias in aliases | {brand}:\n",
    "            alias_norm = ascii_fold(alias)\n",
    "            # ⚙️ patrón flexible: detecta alias pegados (#, @, sin espacios)\n",
    "            pattern = rf\"(?<!\\w){re.escape(alias_norm)}(?!\\w)\"\n",
    "            if re.search(pattern, t):\n",
    "                found.add(brand)\n",
    "                break\n",
    "\n",
    "    # Búsqueda por dominio (URL o columna)\n",
    "    d = (domain or \"\").lower().strip()\n",
    "    d = d.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    d = d.split(\"/\")[0] if d else d\n",
    "    for suf, bname in DOMAIN2BRAND.items():\n",
    "        if suf in t or d.endswith(suf):\n",
    "            found.add(bname)\n",
    "\n",
    "    return sorted(found)\n",
    "\n",
    "def pick_primary_brand(brands: list, text_norm: str) -> str:\n",
    "    \"\"\"Escoge la marca principal por posición de aparición.\"\"\"\n",
    "    if not brands:\n",
    "        return \"\"\n",
    "    t = ascii_fold(text_norm)\n",
    "    positions = []\n",
    "    for b in brands:\n",
    "        aliases = list(BRANDS.get(b, [])) + [b]\n",
    "        pos_min = min([t.find(ascii_fold(a)) for a in aliases if ascii_fold(a) in t] or [10**9])\n",
    "        positions.append((pos_min, b))\n",
    "    positions.sort(key=lambda x: x[0])\n",
    "    return positions[0][1] if positions[0][0] < 10**9 else sorted(brands)[0]\n",
    "\n",
    "# --------- 4) Tokenización -------------------\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "def tokenize_lemma(text_blocked: str) -> list:\n",
    "    doc = nlp(text_blocked)\n",
    "    toks = [tok.lemma_ for tok in doc if not tok.is_space and not tok.is_punct]\n",
    "    return [t for t in toks if t]\n",
    "\n",
    "def tokenize_stem(text_blocked: str) -> list:\n",
    "    doc = nlp(text_blocked)\n",
    "    toks = [stemmer.stem(tok.text.strip()) for tok in doc if not tok.is_space and not tok.is_punct]\n",
    "    return [t for t in toks if t]\n",
    "\n",
    "# --------- 5) Métricas EDA -------------------\n",
    "def ttr(tokens: list) -> float:\n",
    "    return (len(set(tokens)) / max(1, len(tokens))) if tokens else 0.0\n",
    "\n",
    "def hapax_ratio(tokens: list) -> float:\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    from collections import Counter\n",
    "    c = Counter(tokens)\n",
    "    hapax = sum(1 for _, f in c.items() if f == 1)\n",
    "    return hapax / len(c)\n",
    "\n",
    "# ============================================================\n",
    "#  A) CARGA DE DATOS\n",
    "# ============================================================\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET_NAME)\n",
    "if \"id\" not in df.columns.str.lower():\n",
    "    df.insert(0, \"id\", range(1, len(df)+1))\n",
    "\n",
    "df[TEXT_COL] = (df.get(COL_TITLE, \"\").fillna(\"\").astype(str) + \" \" +\n",
    "                df.get(COL_SNIPP, \"\").fillna(\"\").astype(str)).str.strip()\n",
    "\n",
    "if COL_LANG in df.columns:\n",
    "    df = df[df[COL_LANG].str.lower().isin([\"es\", \"español\"])]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "print(f\"Registros cargados: {len(df)}\")\n",
    "\n",
    "# ============================================================\n",
    "#  B) LIMPIEZA + MWE\n",
    "# ============================================================\n",
    "df[\"text_clean_base\"] = df[TEXT_COL].map(basic_clean)\n",
    "df[\"text_mwe_entities\"] = df[\"text_clean_base\"].map(lambda t: block_mwes(t, MWE_ENTITIES))\n",
    "df[\"text_blocked\"] = df[\"text_mwe_entities\"].map(lambda t: block_mwes(t, MWE_SERVICE))\n",
    "\n",
    "# ============================================================\n",
    "#  C) EXTRACCIÓN DE MARCAS\n",
    "# ============================================================\n",
    "df[\"brand_list\"] = df.apply(lambda r: extract_brands(r[\"text_blocked\"], r.get(COL_DOMAIN, \"\")), axis=1)\n",
    "df[\"brand_primary\"] = df.apply(lambda r: pick_primary_brand(r[\"brand_list\"], r[\"text_blocked\"]), axis=1)\n",
    "\n",
    "# ============================================================\n",
    "#  D) TOKENIZACIÓN + TEXTOS\n",
    "# ============================================================\n",
    "df[\"tokens_lemma\"] = df[\"text_blocked\"].map(tokenize_lemma)\n",
    "df[\"tokens_stem\"]  = df[\"text_blocked\"].map(tokenize_stem)\n",
    "\n",
    "def remove_domain_words(tokens: list) -> list:\n",
    "    return [t for t in tokens if t not in DOMAIN_STOPWORDS]\n",
    "\n",
    "df[\"tokens_topics\"] = df[\"tokens_lemma\"].map(remove_domain_words)\n",
    "df[\"text_lemma\"]  = df[\"tokens_lemma\"].map(lambda xs: \" \".join(xs))\n",
    "df[\"text_stem\"]   = df[\"tokens_stem\"].map(lambda xs: \" \".join(xs))\n",
    "df[\"text_topics\"] = df[\"tokens_topics\"].map(lambda xs: \" \".join(xs))\n",
    "\n",
    "# ============================================================\n",
    "#  E) MÉTRICAS EDA\n",
    "# ============================================================\n",
    "df[\"n_chars_raw\"]    = df[TEXT_COL].str.len().fillna(0).astype(int)\n",
    "df[\"n_tokens_lemma\"] = df[\"tokens_lemma\"].map(len)\n",
    "df[\"n_tokens_topics\"]= df[\"tokens_topics\"].map(len)\n",
    "df[\"ttr_lemma\"]      = df[\"tokens_lemma\"].map(ttr)\n",
    "df[\"hapax_lemma\"]    = df[\"tokens_lemma\"].map(hapax_ratio)\n",
    "\n",
    "eda_by_brand = (\n",
    "    df.groupby(\"brand_primary\", dropna=False)\n",
    "      .agg(n_docs=(\"id\",\"count\"),\n",
    "           mean_len=(\"n_tokens_lemma\",\"mean\"),\n",
    "           mean_ttr=(\"ttr_lemma\",\"mean\"))\n",
    "      .sort_values(\"n_docs\", ascending=False)\n",
    ")\n",
    "print(eda_by_brand.head(10))\n",
    "\n",
    "# ============================================================\n",
    "#  F) EXPORT \"CONJUNTO MEDIDO\"\n",
    "# ============================================================\n",
    "cols_measured = [\n",
    "    \"id\", COL_DATE, COL_DOMAIN, COL_URL,\n",
    "    TEXT_COL, \"text_clean_base\", \"text_blocked\",\n",
    "    \"brand_list\", \"brand_primary\",\n",
    "    \"text_lemma\", \"text_stem\", \"text_topics\",\n",
    "    \"n_chars_raw\", \"n_tokens_lemma\", \"n_tokens_topics\",\n",
    "    \"ttr_lemma\", \"hapax_lemma\",\n",
    "]\n",
    "export_df = df[[c for c in cols_measured if c in df.columns]].copy()\n",
    "\n",
    "OUTPUT_CSV = \"corpus_medido2.csv\"\n",
    "OUTPUT_PQ  = \"corpus_medido2.parquet\"\n",
    "\n",
    "export_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "try:\n",
    "    export_df.to_parquet(OUTPUT_PQ, index=False)\n",
    "except Exception as e:\n",
    "    print(\"Parquet no disponible -> solo CSV. Error:\", e)\n",
    "\n",
    "print(f\"Exportado: {OUTPUT_CSV} (y parquet si estuvo disponible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137bd8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['Compañía', 'Fiduciaria', 'La', 'Positiva', 'Previsora', 'Seguros', 'axa', 'bolivar', 'colpatria', 'hdi', 'liberty'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 BIGRAMAS (TEMAS):\n",
      "                ngram  freq   df\n",
      "              https t  7579 3275\n",
      "                 t co  7579 3275\n",
      "       seguro bolivar  2339 1747\n",
      "           grupo argo  2328 1477\n",
      "      junta directivo   752  523\n",
      "      argo grupo_sura   701  529\n",
      "         cemento argo   642  437\n",
      "        axa colpatria   628  463\n",
      "     grupo_sura grupo   536  415\n",
      "      mensaje directo   504  251\n",
      "    ricardo jaramillo   498  332\n",
      "      mensaje privado   478  239\n",
      "      compania seguro   469  361\n",
      " fiduciaria previsora   450  405\n",
      "presidente grupo_sura   430  313\n",
      "    acción grupo_sura   390  301\n",
      "          previsora s   368  324\n",
      "      seguro colombia   356  279\n",
      "         grupo nutrés   354  296\n",
      "       liberty seguro   333  264\n",
      "\n",
      "Top 20 TRIGRAMAS (TEMAS):\n",
      "                                    ngram  freq   df\n",
      "                               https t co  7579 3275\n",
      "                    grupo argo grupo_sura   607  463\n",
      "                    grupo_sura grupo argo   423  315\n",
      "   davidracero positivacol colombiacompra   295  148\n",
      "positivacol colombiacompra wradiocolombia   294  147\n",
      "                   mensaje directo atento   228  115\n",
      "        dcoronell davidracero positivacol   214  107\n",
      "                  favor escribano mensaje   214  107\n",
      "               junta directivo grupo_sura   206  157\n",
      "                    nuevo junta directivo   201  144\n",
      "             ricardo jaramillo presidente   190  129\n",
      "                  nombre numero documento   190   95\n",
      "             aseguramiento integral salud   184   89\n",
      "                    medio mensaje directo   176   88\n",
      "          jaramillo presidente grupo_sura   175  119\n",
      "             grupo empresarial antioqueno   173  138\n",
      "               contactarno pagina oficial   172   86\n",
      "                  pagina oficial facebook   172   86\n",
      "             invitamos contactarno pagina   172   86\n",
      "               oficial facebook instagram   171   86\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TOP N-GRAMAS (bigramas/trigramas)\n",
    "# ============================\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Stopwords español (generales) ---\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "sw_es = set(stopwords.words('spanish'))\n",
    "\n",
    "# Si quieres quitar también tus stopwords de dominio en TEMAS:\n",
    "from collections import Counter\n",
    "# DOMAIN_STOPWORDS viene del bloque anterior (marcas, 'seguros', etc.)\n",
    "sw_temas = sw_es.union(DOMAIN_STOPWORDS)\n",
    "\n",
    "def top_ngrams(texts, ngram_range=(2,2), top_k=50, stopwords_set=None, min_df=2):\n",
    "    \"\"\"\n",
    "    Devuelve un DataFrame con los top n-gramas por frecuencia absoluta.\n",
    "    - texts: iterable de strings\n",
    "    - ngram_range: (2,2) bigramas, (3,3) trigramas, etc.\n",
    "    - stopwords_set: set de stopwords a remover\n",
    "    - min_df: descarta n-gramas que aparecen en < min_df documentos\n",
    "    \"\"\"\n",
    "    if stopwords_set is None:\n",
    "        stopwords_set = set()\n",
    "\n",
    "    # CountVectorizer respeta '_' si usamos token_pattern amplio\n",
    "    vect = CountVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        lowercase=False,\n",
    "        token_pattern=r'(?u)\\b\\w+\\b',\n",
    "        min_df=min_df,\n",
    "        stop_words=list(stopwords_set) if stopwords_set else None\n",
    "    )\n",
    "    X = vect.fit_transform(texts)\n",
    "    freqs = np.asarray(X.sum(axis=0)).ravel()\n",
    "    terms = np.array(vect.get_feature_names_out())\n",
    "    order = np.argsort(freqs)[::-1][:top_k]\n",
    "    out = pd.DataFrame({\n",
    "        \"ngram\": terms[order],\n",
    "        \"freq\": freqs[order],\n",
    "        \"df\": np.asarray((X > 0).sum(axis=0)).ravel()[order]\n",
    "    })\n",
    "    return out\n",
    "\n",
    "# --- 1A) N-gramas de TEMAS (contenido) ---\n",
    "# Usamos text_topics (lemas + MWE de servicio, sin marcas) y quitamos stopwords generales+dominio\n",
    "texts_topics = df[\"text_topics\"].fillna(\"\").astype(str).tolist()\n",
    "top_bi_topics  = top_ngrams(texts_topics, (2,2), top_k=50, stopwords_set=sw_temas, min_df=2)\n",
    "top_tri_topics = top_ngrams(texts_topics, (3,3), top_k=50, stopwords_set=sw_temas, min_df=2)\n",
    "\n",
    "print(\"\\nTop 20 BIGRAMAS (TEMAS):\")\n",
    "print(top_bi_topics.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 20 TRIGRAMAS (TEMAS):\")\n",
    "print(top_tri_topics.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18bb5466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RARE TERMS (<=3): muestra de 30\n",
      "                   term  freq\n",
      "             \"bienestar     1\n",
      "                    \"el     1\n",
      "         \"investigacion     1\n",
      "                   )por     1\n",
      "                 +0,16%     1\n",
      "                  +0,2%     1\n",
      "                 +0,51%     1\n",
      "                  +0,6%     1\n",
      "                 +0,64%     1\n",
      "                  +0,7%     1\n",
      "                 +0,84%     1\n",
      "                  +0,86     1\n",
      "                  +0,89     1\n",
      "                 +0,98%     1\n",
      "       +0.17%118,245.45     1\n",
      "        +0.32%12,540.00     1\n",
      "     +0.33%21,178.58btc     1\n",
      "                 +0.60%     1\n",
      "   +0.70%17,380.00dolar     1\n",
      "+0.86%47,060.00grupoarg     1\n",
      "                 +1,04%     1\n",
      "                 +1,26%     1\n",
      "                 +1,42%     1\n",
      "                 +1,59%     1\n",
      "                  +1,69     1\n",
      "                 +1,71%     1\n",
      "                 +1,96%     1\n",
      "                 +1.03%     1\n",
      "        +1.22%18,137.85     1\n",
      "      +1.26%4,159.67bvc     1\n",
      "Total rare terms (<=3): 19774\n",
      "\n",
      "RARE TERMS que parecen marcas/variantes (muestra 30):\n",
      "                                                                     term  freq\n",
      "                                                               -colpatria     1\n",
      "                                                         0,00%.grupo_sura     1\n",
      "                                                         1.07%.grupo_sura     1\n",
      "                                                         2,45%.grupo_sura     1\n",
      "                                                               2024mapfre     1\n",
      "                                                              agrobolivar     1\n",
      "                                                               allianz.co     1\n",
      "                                                           allianz.sandra     1\n",
      "                                                   allianz15kbarranquillo     1\n",
      "                                                          allianzcolombio     1\n",
      "                                                               apresurado     1\n",
      "                                                              apresurarlo     1\n",
      "                                                              bolivar).se     1\n",
      "                                                        bolivar.categoria     1\n",
      "                                                              bolivar.por     1\n",
      "                                                          bolivar.salario     1\n",
      "                                                                bolivarla     1\n",
      "                                                           bolivarsistemo     1\n",
      "                                                   colombianos.grupo_sura     1\n",
      "colpatria.colmedica.colsanitas.coomeva.medisanitas.medplus.merci.servicio     1\n",
      "                                                     cooperativo).liberty     1\n",
      "                                                               corfinsura     1\n",
      "                                                            fundacionsura     1\n",
      "                                                           fundacionsurar     1\n",
      "                                                         grupo_sura)cenit     1\n",
      "                                                       grupo_sura?noticia     1\n",
      "                                                                 grupsura     1\n",
      "                                              https://www.fundacionmapfre     1\n",
      "                                                       iniciocolombiasura     1\n",
      "                                                               insuranzar     1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_13576\\3704906943.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  looks_like_brand = rare_terms[rare_terms[\"term\"].str.contains(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FORMAS DE MENCIÓN (BIGRAMAS con marca) - Top 30:\n",
      "                     ngram  freq   df\n",
      "              la previsora  2582 1776\n",
      "           seguros bolivar  2342 1750\n",
      "             de grupo_sura  1900 1371\n",
      "              seguros sura  1115  729\n",
      "            del grupo_sura   861  622\n",
      "              grupo_sura y   835  655\n",
      "             axa colpatria   657  483\n",
      "             el grupo_sura   648  491\n",
      "              y grupo_sura   618  460\n",
      "         previsora seguros   541  296\n",
      "       segurosbolivar hola   430  215\n",
      "               previsora s   406  353\n",
      "             en grupo_sura   347  266\n",
      "           liberty seguros   333  264\n",
      "             grupo bolivar   314  263\n",
      "                    sura y   303  241\n",
      "positivacol colombiacompra   300  150\n",
      "   davidracero positivacol   298  149\n",
      "                   de sura   293  236\n",
      "          allianz colombia   274  223\n",
      " davivienda segurosbolivar   248  127\n",
      "                  eps sura   247  159\n",
      "            con grupo_sura   246  185\n",
      "             grupo_sura en   245  184\n",
      "               previsora y   236  146\n",
      "                 de mapfre   224  175\n",
      "               hdi seguros   217  142\n",
      "   segurosbolivar sicsuper   194   98\n",
      "         de segurosbolivar   192   97\n",
      "             sura colombia   192  139\n",
      "\n",
      "FORMAS DE MENCIÓN (TRIGRAMAS con marca) - Top 30:\n",
      "                                    ngram  freq  df\n",
      "                       de seguros bolivar   560 425\n",
      "                          de la previsora   553 368\n",
      "                  fiduciaria la previsora   519 456\n",
      "                       argos y grupo_sura   445 322\n",
      "                            previsora s a   406 353\n",
      "                           la previsora s   403 351\n",
      "                       grupo_sura y grupo   352 274\n",
      "                        y seguros bolivar   343 272\n",
      "                        seguros bolivar y   301 236\n",
      "   davidracero positivacol colombiacompra   298 149\n",
      "                   acciones de grupo_sura   295 228\n",
      "positivacol colombiacompra wradiocolombia   294 147\n",
      "                          de seguros sura   279 191\n",
      "                 presidente de grupo_sura   254 180\n",
      "                           la previsora y   215 127\n",
      "        dcoronell davidracero positivacol   214 107\n",
      "                          de grupo_sura y   197 158\n",
      "                    seguros sura colombia   182 134\n",
      "                           a la previsora   179 147\n",
      "           grupo_sura considera convertir   155 155\n",
      "                presidente del grupo_sura   154 115\n",
      "               diversas formas grupo_sura   152 152\n",
      "                   grupo argos grupo_sura   151 133\n",
      "                           y la previsora   149 103\n",
      "              formas grupo_sura considera   146 146\n",
      "                accionistas de grupo_sura   142 107\n",
      "                 edificio seguros bolivar   137 135\n",
      "                          en la previsora   136  81\n",
      "                       seguros del estado   133 103\n",
      "                     de previsora seguros   131  72\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# PALABRAS POCO FRECUENTES + FORMAS DE MENCIÓN\n",
    "# ============================\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 2A) Rare terms (sin stopwords generales) ---\n",
    "# Usamos tokens \"lemma\" para tener formas comparables, PERO no removemos marcas\n",
    "# (porque queremos ver variantes). Sí quitamos stopwords generales.\n",
    "def rare_terms_from_tokens(list_of_token_lists, stopwords_set, min_len=3, max_freq=3):\n",
    "    \"\"\"\n",
    "    Retorna un DataFrame con términos cuya frecuencia total <= max_freq,\n",
    "    excluyendo stopwords y tokens muy cortos/no alfanuméricos.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for toks in list_of_token_lists:\n",
    "        for t in toks:\n",
    "            if t and len(t) >= min_len and re.search(r\"[a-z0-9áéíóúñ_]\", t):\n",
    "                if t not in stopwords_set:\n",
    "                    counter[t] += 1\n",
    "    items = [(t, f) for t, f in counter.items() if f <= max_freq]\n",
    "    out = pd.DataFrame(items, columns=[\"term\", \"freq\"]).sort_values([\"freq\",\"term\"])\n",
    "    return out\n",
    "\n",
    "rare_terms = rare_terms_from_tokens(\n",
    "    df[\"tokens_lemma\"].apply(lambda x: x if isinstance(x, list) else []).tolist(),\n",
    "    stopwords_set=sw_es,\n",
    "    min_len=3,\n",
    "    max_freq=3,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nRARE TERMS (<=3): muestra de 30\")\n",
    "print(rare_terms.head(30).to_string(index=False))\n",
    "print(f\"Total rare terms (<=3): {len(rare_terms)}\")\n",
    "\n",
    "# Puedes filtrar candidates que \"suenen a marca\" (porvenir, sura, bolivar, mapfre, allianz...)\n",
    "looks_like_brand = rare_terms[rare_terms[\"term\"].str.contains(\n",
    "    r\"(sura|bolivar|bolívar|mapfre|allianz|colpatria|zurich|liberty|hdi|porvenir)\",\n",
    "    case=False, regex=True\n",
    ")]\n",
    "print(\"\\nRARE TERMS que parecen marcas/variantes (muestra 30):\")\n",
    "print(looks_like_brand.head(30).to_string(index=False))\n",
    "\n",
    "\n",
    "# --- 2B) N-gramas que incluyen una marca (formas de mención) ---\n",
    "# Tomamos text_blocked (con MWE unidas por '_') para capturar cosas como 'grupo_sura'\n",
    "texts_blocked = df[\"text_blocked\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# Construimos un set plano de alias (ya está en minúsculas en tu pipeline)\n",
    "brand_alias_flat = set()\n",
    "for brand, aliases in BRANDS.items():\n",
    "    brand_alias_flat.add(brand)\n",
    "    for a in aliases:\n",
    "        brand_alias_flat.add(a.lower())\n",
    "\n",
    "# Un pequeño vectorizador SIN stopwords para no perder marcas\n",
    "def ngram_counts_including_brands(texts, ngram_range=(2,2), min_df=1):\n",
    "    vect = CountVectorizer(\n",
    "        ngram_range=ngram_range, lowercase=False,\n",
    "        token_pattern=r'(?u)\\b\\w+\\b', min_df=min_df\n",
    "    )\n",
    "    X = vect.fit_transform(texts)\n",
    "    terms = np.array(vect.get_feature_names_out())\n",
    "    freqs = np.asarray(X.sum(axis=0)).ravel()\n",
    "    dfs   = np.asarray((X > 0).sum(axis=0)).ravel()\n",
    "    df_terms = pd.DataFrame({\"ngram\": terms, \"freq\": freqs, \"df\": dfs})\n",
    "    # filtra los n-gramas que contienen algún alias/marca\n",
    "    mask = df_terms[\"ngram\"].apply(\n",
    "        lambda s: any(b in s for b in brand_alias_flat)\n",
    "    )\n",
    "    return df_terms[mask].sort_values(\"freq\", ascending=False)\n",
    "\n",
    "brand_bigrams  = ngram_counts_including_brands(texts_blocked, (2,2), min_df=1)\n",
    "brand_trigrams = ngram_counts_including_brands(texts_blocked, (3,3), min_df=1)\n",
    "\n",
    "print(\"\\nFORMAS DE MENCIÓN (BIGRAMAS con marca) - Top 30:\")\n",
    "print(brand_bigrams.head(30).to_string(index=False))\n",
    "\n",
    "print(\"\\nFORMAS DE MENCIÓN (TRIGRAMAS con marca) - Top 30:\")\n",
    "print(brand_trigrams.head(30).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ef5cb",
   "metadata": {},
   "source": [
    "Primeros Ajustes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11101e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1A) Ampliar MWE (servicio) y entidades/noticioso/CM ---\n",
    "MWE_SERVICE += [\n",
    "    \"pago de siniestro\",\"pago de la póliza\",\"tiempo de respuesta\",\"atención al cliente\",\n",
    "    \"servicio al cliente\",\"línea de atención\",\"red médica\",\"autorización de servicio\",\n",
    "    \"incremento de prima\",\"renovación de póliza\",\"radicado de pqrs\",\"letra pequeña\"\n",
    "]\n",
    "MWE_ENTITIES += [\n",
    "    \"grupo argos\",\"grupo sura\",\"grupo empresarial antioqueño\",\"suramericana s a\",\n",
    "    \"inversiones suramericana\",\"fundacion mapfre\",\"allianz colombia\",\"seguros del estado\"\n",
    "]\n",
    "\n",
    "# --- 1B) Alias/typos para marcas (gazetteer) ---\n",
    "BRANDS[\"seguros bolivar\"] |= {\n",
    "    \"seguros bolívar\",\"segurosbolivar\",\"#segurosbolivar\",\"#segurosbolívar\",\n",
    "    \"grupo bolívar\",\"grupo bolivar\",\"grupobolivar\",\"agrobolivar\",\"bolivar seguros\"\n",
    "}\n",
    "BRANDS[\"sura\"] |= {\n",
    "    \"grupo sura\",\"gruposura\",\"grup_sura\",\"grupsura\",\"suramericana\",\"eps sura\",\n",
    "    \"inversiones suramericana\",\"suramericana s a\",\"@suramericana\",\"segurossura\"\n",
    "}\n",
    "BRANDS[\"mapfre\"] |= {\"fundacion mapfre\",\"mapfre col\",\"mapfre_col\",\"2024mapfre\"}\n",
    "BRANDS[\"allianz\"] |= {\"allianz colombia\",\"allianz.co\",\"allianzcol\",\"allianzcolombio\"}\n",
    "BRANDS[\"axa colpatria\"] |= {\"axacolpatria\",\"colpatria seguros\",\"axacolpatriar\"}\n",
    "BRANDS[\"liberty seguros\"] |= {\"liberty seguros\",\"legal-liberty\"}\n",
    "BRANDS[\"seguros del estado\"] |= {\"seguros del estado\"}\n",
    "BRANDS[\"zurich\"] |= {\"zurich colombia\"}\n",
    "BRANDS[\"hdi seguros\"] |= {\"hdi seguros\"}\n",
    "BRANDS[\"porvenir\"] |= {\"porvenir\",\"fondo de pensiones obligatorias porvenir\"}\n",
    "\n",
    "# --- 1C) Dominios por sufijo ---\n",
    "DOMAIN_SUFFIX2BRAND = {\n",
    "    \"segurosbolivar.com\":\"seguros bolivar\",\"grupobolivar.com\":\"seguros bolivar\",\n",
    "    \"segurossura.com\":\"sura\",\"sura.com\":\"sura\",\"epssura.com\":\"sura\",\n",
    "    \"axacolpatria.co\":\"axa colpatria\",\"allianz.co\":\"allianz\",\"mapfre.com\":\"mapfre\",\n",
    "    \"fundacionmapfre.org\":\"mapfre\",\"libertycolombia.com\":\"liberty seguros\",\n",
    "    \"zurich.com.co\":\"zurich\",\"hdi.com.co\":\"hdi seguros\",\"segurosdelestado.com\":\"seguros del estado\",\n",
    "    \"porvenir.com.co\":\"porvenir\"\n",
    "}\n",
    "\n",
    "# --- 1D) Stopwords de dominio (solo para TEMAS) ---\n",
    "DOMAIN_STOPWORDS |= {\n",
    "    \"seguro\",\"seguros\",\"aseguradora\",\"aseguradoras\",\"poliza\",\"póliza\",\"cliente\",\"clientes\",\n",
    "    \"grupo\",\"fundacion\",\"fundación\",\"colombia\",\"colombiano\",\"colombianos\",\n",
    "    *set(BRANDS.keys())\n",
    "}\n",
    "\n",
    "# --- 1E) Normalizaciones canónicas antes de tokenizar ---\n",
    "CANON_REPL = [\n",
    "    (r\"\\bjunta directivo\\b\",\"junta directiva\"),\n",
    "    (r\"\\bcemento argo\\b\",\"cementos argos\"),\n",
    "    (r\"\\bprevisora s\\b\",\"previsora s.a.\"),\n",
    "    (r\"\\bseguro bolivar\\b\",\"seguros bolivar\"),\n",
    "    (r\"\\bseguro surar\\b\",\"seguros sura\"),\n",
    "    (r\"\\bcompan[ií]a seguro\\b\",\"compañía de seguros\"),\n",
    "    (r\"\\bgrupo nutr[ée]s?\\b\",\"grupo nutresa\"),\n",
    "]\n",
    "def canonicalize(text: str) -> str:\n",
    "    t = text\n",
    "    for pat, rep in CANON_REPL:\n",
    "        t = re.sub(pat, rep, t, flags=re.IGNORECASE)\n",
    "    return t\n",
    "\n",
    "# --- 1F) Flags para filtrar corporativo y respuestas de CM ---\n",
    "CORP_NEWS_PATTERNS = {\n",
    "    \"grupo_sura\",\"grupo argos\",\"argos grupo\",\"junta directiv\",\"presidente\",\n",
    "    \"grupo empresarial antioque\",\"acciones\",\"accionistas\",\n",
    "    \"inversiones suramericana\",\"suramericana s a\",\"edificio seguros bolivar\"\n",
    "}\n",
    "CM_TEMPLATES = {\n",
    "    \"mensaje directo\",\"mensaje privado\",\"favor escrib\",\"invitamos contactar\",\n",
    "    \"pagina oficial\",\"facebook\",\"instagram\",\"contactarno pagina\",\"medio mensaje directo\"\n",
    "}\n",
    "def is_corporate_news(t: str) -> bool:\n",
    "    return any(pat in ascii_fold(t) for pat in CORP_NEWS_PATTERNS)\n",
    "def is_cm_template(t: str) -> bool:\n",
    "    return any(pat in ascii_fold(t) for pat in CM_TEMPLATES)\n",
    "\n",
    "# --- 1G) extract_brands actualizado (texto + dominio por sufijo) ---\n",
    "def extract_brands(text_norm: str, domain: str = \"\") -> list:\n",
    "    found = set()\n",
    "    t = \" \" + ascii_fold(text_norm) + \" \"\n",
    "    for brand, aliases in BRANDS.items():\n",
    "        if any((\" \" + ascii_fold(a) + \" \") in t for a in (aliases | {brand})):\n",
    "            found.add(brand)\n",
    "    d = (domain or \"\").lower().strip()\n",
    "    d = d.replace(\"https://\",\"\").replace(\"http://\",\"\").split(\"/\")[0]\n",
    "    d = d[4:] if d.startswith(\"www.\") else d\n",
    "    for suffix, brand in DOMAIN_SUFFIX2BRAND.items():\n",
    "        if d.endswith(suffix):\n",
    "            found.add(brand)\n",
    "    return sorted(found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2895d71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15469, 10684)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-aplica limpieza canónica\n",
    "df[\"text_clean_base\"] = df[\"text_clean_base\"].map(canonicalize)\n",
    "\n",
    "# Re-bloquea MWE (entidades primero, luego servicio)\n",
    "df[\"text_mwe_entities\"] = df[\"text_clean_base\"].map(lambda t: block_mwes(t, MWE_ENTITIES))\n",
    "df[\"text_blocked\"] = df[\"text_mwe_entities\"].map(lambda t: block_mwes(t, MWE_SERVICE))\n",
    "\n",
    "# Recalcula marcas\n",
    "df[\"brand_list\"] = df.apply(lambda r: extract_brands(r[\"text_blocked\"], r.get(COL_DOMAIN, \"\")), axis=1)\n",
    "df[\"brand_primary\"] = df.apply(lambda r: pick_primary_brand(r[\"brand_list\"], r[\"text_blocked\"]), axis=1)\n",
    "\n",
    "# Flags corporativo/CM\n",
    "df[\"flag_corp_news\"] = df[\"text_clean_base\"].map(is_corporate_news)\n",
    "df[\"flag_cm_reply\"]  = df[\"text_clean_base\"].map(is_cm_template)\n",
    "\n",
    "# Tokens y textos para TEMAS (lema + sin marcas)\n",
    "df[\"tokens_lemma\"] = df[\"text_blocked\"].map(tokenize_lemma)\n",
    "df[\"tokens_topics\"] = df[\"tokens_lemma\"].map(lambda xs: [t for t in xs if t not in DOMAIN_STOPWORDS])\n",
    "df[\"text_topics\"]   = df[\"tokens_topics\"].map(lambda xs: \" \".join(xs))\n",
    "\n",
    "# Subconjunto tema-centrado (sug.) para sacar n-gramas “limpios”\n",
    "df_topics = df[~df[\"flag_corp_news\"] & ~df[\"flag_cm_reply\"]].copy()\n",
    "len(df), len(df_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c445efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['Compañía', 'Fiduciaria', 'La', 'Positiva', 'Previsora', 'Seguros', 'axa', 'bolivar', 'colpatria', 'hdi', 'liberty'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 BIGRAMAS (TEMAS, filtrado corporativo/CM):\n",
      "                        ngram  freq   df\n",
      "                         t co  5835 2462\n",
      "                      https t  5835 2462\n",
      "   positivacol colombiacompra   300  150\n",
      "      davidracero positivacol   293  146\n",
      "colombiacompra wradiocolombia   290  145\n",
      "                sistema salud   222  151\n",
      "        dcoronell davidracero   210  105\n",
      "                    nuevo eps   199  106\n",
      "                       si ser   191  116\n",
      "                      ser mas   189  125\n",
      "            primero trimestre   186  119\n",
      "               integral salud   180   87\n",
      "       aseguramiento integral   180   87\n",
      "             primero semestre   175  118\n",
      "                utilidad neta   171  111\n",
      "           patinaje velocidad   167   84\n",
      "   daviviendir segurosbolivar   165   92\n",
      "           davivienda bolivar   164  140\n",
      "                    haber ser   159  116\n",
      "               riesgo laboral   144   76\n",
      "\n",
      "Top 20 TRIGRAMAS (TEMAS, filtrado corporativo/CM):\n",
      "                                    ngram  freq   df\n",
      "                               https t co  5835 2462\n",
      "   davidracero positivacol colombiacompra   291  146\n",
      "positivacol colombiacompra wradiocolombia   290  145\n",
      "        dcoronell davidracero positivacol   210  105\n",
      "             aseguramiento integral salud   176   85\n",
      "             senor supersalud positivacol   112   57\n",
      "                    primero semestre 2024   109   78\n",
      "                           value and risk   103   52\n",
      "                  nacional gestion riesgo    91   84\n",
      "                  2024 patinaje velocidad    82   41\n",
      "                       nuevo modelo salud    81   61\n",
      "          modelo sostenible aseguramiento    78   39\n",
      "                    solventar caso manera    76   38\n",
      "                       caso manera rapido    76   38\n",
      "                     poder solventar caso    76   38\n",
      "                  seguridad salud trabajo    75   46\n",
      "                     manera rapido eficaz    74   37\n",
      "                          dej alguno dato    74   37\n",
      "                 atenderte manera directo    74   37\n",
      "                        alguno dato poder    74   37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_13576\\1020284944.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  looks_like_brand = rare_terms[rare_terms[\"term\"].str.contains(brand_regex, case=False, regex=True, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RARE TERMS que parecen marcas/variantes (muestra 30):\n",
      "                                                                         term  freq\n",
      "                                                               -axa_colpatria     1\n",
      "                                                                   -colpatria     1\n",
      "                                                                 -grupo_argos     1\n",
      "                                                             0,00%.grupo_sura     1\n",
      "                                                             1.07%.grupo_sura     1\n",
      "                                                             2,45%.grupo_sura     1\n",
      "                                                                   2024mapfre     1\n",
      "                                                                  agrobolivar     1\n",
      "                                                                   allianz.co     1\n",
      "                                                               allianz.sandra     1\n",
      "                                                       allianz15kbarranquillo     1\n",
      "                                                              allianzcolombio     1\n",
      "                                                                   apresurado     1\n",
      "                                                                  apresurarlo     1\n",
      "                                                                        argos     1\n",
      "                                                                   argos-suro     1\n",
      "                                                                      argos16     1\n",
      "                                                                      argos27     1\n",
      "                                                                    argos?que     1\n",
      "                                                               argoseconomiar     1\n",
      "                                               argosempresassuperintendenciar     1\n",
      "                                                                   argosgrupo     1\n",
      "                                                               argosodinsas&p     1\n",
      "                                                               argosresultado     1\n",
      "axa_colpatria.colmedica.colsanitas.coomeva.medisanitas.medplus.merci.servicio     1\n",
      "                                                                  bolivar).se     1\n",
      "                                                            bolivar.categoria     1\n",
      "                                                                  bolivar.por     1\n",
      "                                                              bolivar.salario     1\n",
      "                                                                    bolivarla     1\n",
      "\n",
      "FORMAS DE MENCIÓN (BIGRAMAS con marca) - Top 30:\n",
      "                     ngram  freq   df\n",
      "           seguros bolivar  2342 1750\n",
      "              la previsora  2194 1448\n",
      "             de grupo_sura  1900 1371\n",
      "              seguros sura  1115  729\n",
      "            del grupo_sura   861  622\n",
      "              grupo_sura y   835  655\n",
      "             el grupo_sura   648  491\n",
      "              y grupo_sura   618  460\n",
      "         previsora seguros   541  296\n",
      "       segurosbolivar hola   430  215\n",
      "             previsora_s a   391  338\n",
      "            la previsora_s   388  336\n",
      "             en grupo_sura   347  266\n",
      "           liberty seguros   333  264\n",
      "             grupo bolivar   314  263\n",
      "                    sura y   303  241\n",
      "positivacol colombiacompra   300  150\n",
      "   davidracero positivacol   298  149\n",
      "                   de sura   293  236\n",
      " davivienda segurosbolivar   248  127\n",
      "                  eps sura   247  159\n",
      "            con grupo_sura   246  185\n",
      "             grupo_sura en   245  184\n",
      "               previsora y   236  146\n",
      "                 de mapfre   224  175\n",
      "               hdi seguros   217  142\n",
      "   segurosbolivar sicsuper   194   98\n",
      "         de segurosbolivar   192   97\n",
      "             sura colombia   192  139\n",
      "            grupo_sura que   178  143\n",
      "\n",
      "FORMAS DE MENCIÓN (TRIGRAMAS con marca) - Top 30:\n",
      "                                    ngram  freq  df\n",
      "                       de seguros bolivar   560 425\n",
      "                          de la previsora   497 316\n",
      "                 grupo_argos y grupo_sura   408 294\n",
      "                          previsora_s a a   391 338\n",
      "                         la previsora_s a   388 336\n",
      "                        y seguros bolivar   343 272\n",
      "                  fiduciaria la previsora   333 301\n",
      "                        seguros bolivar y   301 236\n",
      "   davidracero positivacol colombiacompra   298 149\n",
      "                 grupo_sura y grupo_argos   298 227\n",
      "                   acciones de grupo_sura   295 228\n",
      "positivacol colombiacompra wradiocolombia   294 147\n",
      "                          de seguros sura   279 191\n",
      "                 presidente de grupo_sura   254 180\n",
      "                           la previsora y   215 127\n",
      "        dcoronell davidracero positivacol   214 107\n",
      "                          de grupo_sura y   197 158\n",
      "                fiduciaria la previsora_s   186 160\n",
      "                    seguros sura colombia   182 134\n",
      "           grupo_sura considera convertir   155 155\n",
      "                presidente del grupo_sura   154 115\n",
      "               diversas formas grupo_sura   152 152\n",
      "                           a la previsora   148 117\n",
      "              formas grupo_sura considera   146 146\n",
      "                accionistas de grupo_sura   142 107\n",
      "                 edificio seguros bolivar   137 135\n",
      "                          en la previsora   133  78\n",
      "                     de previsora seguros   131  72\n",
      "                           y la previsora   128  85\n",
      "                       en seguros bolivar   122  91\n"
     ]
    }
   ],
   "source": [
    "# --- TOP N-GRAMAS (TEMAS) sobre df_topics ---\n",
    "texts_topics = df_topics[\"text_topics\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "top_bi_topics  = top_ngrams(texts_topics, (2,2), top_k=50, stopwords_set=sw_temas, min_df=2)\n",
    "top_tri_topics = top_ngrams(texts_topics, (3,3), top_k=50, stopwords_set=sw_temas, min_df=2)\n",
    "\n",
    "print(\"\\nTop 20 BIGRAMAS (TEMAS, filtrado corporativo/CM):\")\n",
    "print(top_bi_topics.head(20).to_string(index=False))\n",
    "print(\"\\nTop 20 TRIGRAMAS (TEMAS, filtrado corporativo/CM):\")\n",
    "print(top_tri_topics.head(20).to_string(index=False))\n",
    "\n",
    "# --- RARE TERMS (para detectar alias) en df (global) ---\n",
    "brand_regex = r\"(sura|bol[ií]var|mapfre|allianz|colpatria|zurich|liberty|hdi|porvenir|suramericana|argos)\"\n",
    "token_lists = df[\"tokens_lemma\"].apply(\n",
    "    lambda x: x if isinstance(x, list)\n",
    "    else ([] if pd.isna(x) else (x.split() if isinstance(x, str) else []))\n",
    ")\n",
    "rare_terms = rare_terms_from_tokens(\n",
    "    token_lists.tolist(),\n",
    "    stopwords_set=sw_es,\n",
    "    min_len=3,\n",
    "    max_freq=3\n",
    ")\n",
    "looks_like_brand = rare_terms[rare_terms[\"term\"].str.contains(brand_regex, case=False, regex=True, na=False)]\n",
    "print(\"\\nRARE TERMS que parecen marcas/variantes (muestra 30):\")\n",
    "print(looks_like_brand.head(30).to_string(index=False))\n",
    "\n",
    "# --- FORMAS DE MENCIÓN (n-gramas con marca) sobre df (global) ---\n",
    "texts_blocked = df[\"text_blocked\"].fillna(\"\").astype(str).tolist()\n",
    "brand_bigrams  = ngram_counts_including_brands(texts_blocked, (2,2), min_df=1)\n",
    "brand_trigrams = ngram_counts_including_brands(texts_blocked, (3,3), min_df=1)\n",
    "\n",
    "print(\"\\nFORMAS DE MENCIÓN (BIGRAMAS con marca) - Top 30:\")\n",
    "print(brand_bigrams.head(30).to_string(index=False))\n",
    "print(\"\\nFORMAS DE MENCIÓN (TRIGRAMAS con marca) - Top 30:\")\n",
    "print(brand_trigrams.head(30).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9671b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ENTIDADES (bloqueo como unidad, no para TEMAS) ---\n",
    "MWE_ENTITIES += [\n",
    "    \"fiduciaria previsora\", \"previsora s.a.\", \"axa colpatria\"\n",
    "]\n",
    "\n",
    "# --- BRANDS (alias/typos detectados) ---\n",
    "BRANDS.setdefault(\"fiduciaria previsora\", set())\n",
    "BRANDS[\"fiduciaria previsora\"] |= {\"fiduciaria previsora\", \"previsora s\", \"previsora s.a.\", \"previsora\"}\n",
    "\n",
    "BRANDS[\"axa colpatria\"] |= {\"axacolpatria\", \"-colpatria\"}  # aparece en rare terms\n",
    "\n",
    "DOMAIN_STOPWORDS |= {\n",
    "    \"fiduciaria\", \"previsora\", \"previsora s.a.\", \"axa\", \"colpatria\", \"axa colpatria\"\n",
    "}\n",
    "# Amplía los patrones de CM\n",
    "CM_TEMPLATES |= {\n",
    "    \"dejar link\", \"link dej\", \"aqui dejar link\",\n",
    "    \"atenderte manera\", \"atenderte manera directo\",\n",
    "    \"herramienta atenderte\", \"solventar caso\", \"poder solventar caso\",\n",
    "    \"medio mensaje directo\", \"favor escribano mensaje\", \"mensaje directo atento\"\n",
    "}\n",
    "\n",
    "MWE_ENTITIES += [\n",
    "    \"dejar link\", \"atenderte manera directo\", \"herramienta atenderte manera\",\n",
    "    \"solventar caso manera\"\n",
    "]\n",
    "\n",
    "CORP_NEWS_PATTERNS |= {\n",
    "    \"utilidad neta\", \"primero trimestre\", \"primer trimestre\",\n",
    "    \"primero semestre\", \"primer semestre\", \"value and risk\",\n",
    "    \"patinaje velocidad\", \"davivienda bolivar\", \"nuevo modelo salud\",\n",
    "    \"aseguramiento integral salud\", \"sistema salud\", \"integral salud\",\n",
    "    \"seguridad salud trabajo\", \"fondo nacional\", \"gestion riesgo\", \"riesgo laboral\"\n",
    "}\n",
    "\n",
    "STOPWORDS_EXTRA = {\n",
    "    \"ser\",\"haber\",\"poder\",\"mas\",\"si\",\"nuevo\",\"empresa\",\"nacional\",\"gestión\",\"riesgo\",\"integral\"\n",
    "}\n",
    "DOMAIN_STOPWORDS |= STOPWORDS_EXTRA\n",
    "\n",
    "CANON_REPL += [\n",
    "    (r\"\\bprevisora s\\b\",\"previsora s.a.\"),\n",
    "    (r\"\\bvalue and risk\\b\",\"value and risk\"),  # si decides filtrar, basta con el patrón en CORP_NEWS_PATTERNS\n",
    "    (r\"\\bprimero semestre\\b\",\"primer semestre\"),\n",
    "    (r\"\\bprimero trimestre\\b\",\"primer trimestre\"),\n",
    "]\n",
    "\n",
    "NUMPCT_TOKEN_RE = re.compile(r\"(?=.*\\d)(?=.*[%\\.,])\")  # contiene dígitos y %/./, mezclados\n",
    "\n",
    "def remove_num_pct_tokens(tokens):\n",
    "    return [t for t in tokens if not NUMPCT_TOKEN_RE.search(t)]\n",
    "\n",
    "# aplica sólo a TEMAS:\n",
    "df[\"tokens_topics\"] = df[\"tokens_lemma\"].map(lambda xs: remove_num_pct_tokens(\n",
    "    [t for t in xs if t not in DOMAIN_STOPWORDS]\n",
    "))\n",
    "df[\"text_topics\"] = df[\"tokens_topics\"].map(lambda xs: \" \".join(xs))\n",
    "\n",
    "NUMPCT_TOKEN_RE = re.compile(r\"(?=.*\\d)(?=.*[%\\.,])\")  # contiene dígitos y %/./, mezclados\n",
    "\n",
    "def remove_num_pct_tokens(tokens):\n",
    "    return [t for t in tokens if not NUMPCT_TOKEN_RE.search(t)]\n",
    "\n",
    "# aplica sólo a TEMAS:\n",
    "df[\"tokens_topics\"] = df[\"tokens_lemma\"].map(lambda xs: remove_num_pct_tokens(\n",
    "    [t for t in xs if t not in DOMAIN_STOPWORDS]\n",
    "))\n",
    "df[\"text_topics\"] = df[\"tokens_topics\"].map(lambda xs: \" \".join(xs))\n",
    "\n",
    "# Reaplica canonicalize\n",
    "df[\"text_clean_base\"] = df[\"text_clean_base\"].map(canonicalize)\n",
    "\n",
    "# Rebloquea MWE\n",
    "df[\"text_mwe_entities\"] = df[\"text_clean_base\"].map(lambda t: block_mwes(t, MWE_ENTITIES))\n",
    "df[\"text_blocked\"] = df[\"text_mwe_entities\"].map(lambda t: block_mwes(t, MWE_SERVICE))\n",
    "\n",
    "# Recalcula marcas + flags\n",
    "df[\"brand_list\"] = df.apply(lambda r: extract_brands(r[\"text_blocked\"], r.get(COL_DOMAIN, \"\")), axis=1)\n",
    "df[\"brand_primary\"] = df.apply(lambda r: pick_primary_brand(r[\"brand_list\"], r[\"text_blocked\"]), axis=1)\n",
    "df[\"flag_corp_news\"] = df[\"text_clean_base\"].map(is_corporate_news)\n",
    "df[\"flag_cm_reply\"]  = df[\"text_clean_base\"].map(is_cm_template)\n",
    "\n",
    "# Tokens/temas\n",
    "df[\"tokens_lemma\"] = df[\"text_blocked\"].map(tokenize_lemma)\n",
    "df[\"tokens_topics\"] = df[\"tokens_lemma\"].map(lambda xs: remove_num_pct_tokens([t for t in xs if t not in DOMAIN_STOPWORDS]))\n",
    "df[\"text_topics\"]   = df[\"tokens_topics\"].map(lambda xs: \" \".join(xs))\n",
    "\n",
    "# Subconjunto tema-centrado\n",
    "df_topics = df[~df[\"flag_corp_news\"] & ~df[\"flag_cm_reply\"]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fefeaba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['Compañía', 'Fiduciaria', 'La', 'Positiva', 'Previsora', 'Seguros', 'axa', 'bolivar', 'colpatria', 'hdi', 'liberty'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 BIGRAMAS (TEMAS, filtrado corporativo/CM):\n",
      "                        ngram  freq  df\n",
      "                      https t   971 469\n",
      "                         t co   971 469\n",
      "   positivacol colombiacompra   300 150\n",
      "      davidracero positivacol   293 146\n",
      "colombiacompra wradiocolombia   290 145\n",
      "        dcoronell davidracero   210 105\n",
      "          aseguramiento salud   208 100\n",
      "                sistema salud   208 140\n",
      "           patinaje velocidad   167  84\n",
      "   daviviendir segurosbolivar   165  92\n",
      "           davivienda bolivar   162 138\n",
      "                salud bolivar   131  81\n",
      "                 modelo salud   127  94\n",
      "       supersalud positivacol   125  65\n",
      "     daviplata segurosbolivar   125  65\n",
      "                 salud mental   122  66\n",
      "    segurosbolivar davivienda   122  62\n",
      "         modelo aseguramiento   120  48\n",
      "                 millón pesos   117  97\n",
      "             senor supersalud   116  59\n",
      "\n",
      "Top 20 TRIGRAMAS (TEMAS, filtrado corporativo/CM):\n",
      "                                    ngram  freq  df\n",
      "                               https t co   971 469\n",
      "   davidracero positivacol colombiacompra   291 146\n",
      "positivacol colombiacompra wradiocolombia   290 145\n",
      "        dcoronell davidracero positivacol   210 105\n",
      "             senor supersalud positivacol   112  57\n",
      "               modelo aseguramiento salud    96  47\n",
      "                  2024 patinaje velocidad    82  41\n",
      "          modelo sostenible aseguramiento    78  39\n",
      "                    solventar caso manera    76  38\n",
      "                       caso manera rapido    76  38\n",
      "                  seguridad salud trabajo    75  46\n",
      "                      dato solventar caso    74  37\n",
      "                          dej alguno dato    74  37\n",
      "                 atenderte manera directo    74  37\n",
      "                    alguno dato solventar    74  37\n",
      "                           dejar link dej    74  37\n",
      "                      manera directo aqui    74  37\n",
      "                     manera rapido eficaz    74  37\n",
      "                       directo aqui dejar    74  37\n",
      "             herramienta atenderte manera    74  37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_13576\\350065347.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  looks_like_brand = rare_terms[rare_terms[\"term\"].str.contains(brand_regex, case=False, regex=True, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RARE TERMS que parecen marcas/variantes (muestra 30):\n",
      "                                                                         term  freq\n",
      "                                                               -axa_colpatria     1\n",
      "                                                                   -colpatria     1\n",
      "                                                                 -grupo_argos     1\n",
      "                                                             0,00%.grupo_sura     1\n",
      "                                                             1.07%.grupo_sura     1\n",
      "                                                             2,45%.grupo_sura     1\n",
      "                                                                   2024mapfre     1\n",
      "                                                                  agrobolivar     1\n",
      "                                                                   allianz.co     1\n",
      "                                                               allianz.sandra     1\n",
      "                                                       allianz15kbarranquillo     1\n",
      "                                                              allianzcolombio     1\n",
      "                                                                   apresurado     1\n",
      "                                                                  apresurarlo     1\n",
      "                                                                        argos     1\n",
      "                                                                   argos-suro     1\n",
      "                                                                      argos16     1\n",
      "                                                                      argos27     1\n",
      "                                                                    argos?que     1\n",
      "                                                               argoseconomiar     1\n",
      "                                               argosempresassuperintendenciar     1\n",
      "                                                                   argosgrupo     1\n",
      "                                                               argosodinsas&p     1\n",
      "                                                               argosresultado     1\n",
      "axa_colpatria.colmedica.colsanitas.coomeva.medisanitas.medplus.merci.servicio     1\n",
      "                                                                  bolivar).se     1\n",
      "                                                            bolivar.categoria     1\n",
      "                                                                  bolivar.por     1\n",
      "                                                              bolivar.salario     1\n",
      "                                                                    bolivarla     1\n",
      "\n",
      "FORMAS DE MENCIÓN (BIGRAMAS con marca) - Top 30:\n",
      "                     ngram  freq   df\n",
      "           seguros bolivar  2342 1750\n",
      "              la previsora  2194 1448\n",
      "             de grupo_sura  1900 1371\n",
      "              seguros sura  1115  729\n",
      "            del grupo_sura   861  622\n",
      "              grupo_sura y   835  655\n",
      "             el grupo_sura   648  491\n",
      "              y grupo_sura   618  460\n",
      "         previsora seguros   541  296\n",
      "       segurosbolivar hola   430  215\n",
      "             previsora_s a   391  338\n",
      "            la previsora_s   388  336\n",
      "             en grupo_sura   347  266\n",
      "           liberty seguros   333  264\n",
      "             grupo bolivar   314  263\n",
      "                    sura y   303  241\n",
      "positivacol colombiacompra   300  150\n",
      "   davidracero positivacol   298  149\n",
      "                   de sura   293  236\n",
      " davivienda segurosbolivar   248  127\n",
      "                  eps sura   247  159\n",
      "            con grupo_sura   246  185\n",
      "             grupo_sura en   245  184\n",
      "               previsora y   236  146\n",
      "                 de mapfre   224  175\n",
      "               hdi seguros   217  142\n",
      "   segurosbolivar sicsuper   194   98\n",
      "         de segurosbolivar   192   97\n",
      "             sura colombia   192  139\n",
      "            grupo_sura que   178  143\n",
      "\n",
      "FORMAS DE MENCIÓN (TRIGRAMAS con marca) - Top 30:\n",
      "                                    ngram  freq  df\n",
      "                       de seguros bolivar   560 425\n",
      "                          de la previsora   497 316\n",
      "                 grupo_argos y grupo_sura   408 294\n",
      "                          previsora_s a a   391 338\n",
      "                         la previsora_s a   388 336\n",
      "                        y seguros bolivar   343 272\n",
      "                  fiduciaria la previsora   333 301\n",
      "                        seguros bolivar y   301 236\n",
      "   davidracero positivacol colombiacompra   298 149\n",
      "                 grupo_sura y grupo_argos   298 227\n",
      "                   acciones de grupo_sura   295 228\n",
      "positivacol colombiacompra wradiocolombia   294 147\n",
      "                          de seguros sura   279 191\n",
      "                 presidente de grupo_sura   254 180\n",
      "                           la previsora y   215 127\n",
      "        dcoronell davidracero positivacol   214 107\n",
      "                          de grupo_sura y   197 158\n",
      "                fiduciaria la previsora_s   186 160\n",
      "                    seguros sura colombia   182 134\n",
      "           grupo_sura considera convertir   155 155\n",
      "                presidente del grupo_sura   154 115\n",
      "               diversas formas grupo_sura   152 152\n",
      "                           a la previsora   148 117\n",
      "              formas grupo_sura considera   146 146\n",
      "                accionistas de grupo_sura   142 107\n",
      "                 edificio seguros bolivar   137 135\n",
      "                          en la previsora   133  78\n",
      "                     de previsora seguros   131  72\n",
      "                           y la previsora   128  85\n",
      "                       en seguros bolivar   122  91\n"
     ]
    }
   ],
   "source": [
    "# --- TOP N-GRAMAS (TEMAS) sobre df_topics ---\n",
    "texts_topics = df_topics[\"text_topics\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "top_bi_topics  = top_ngrams(texts_topics, (2,2), top_k=50, stopwords_set=sw_temas, min_df=2)\n",
    "top_tri_topics = top_ngrams(texts_topics, (3,3), top_k=50, stopwords_set=sw_temas, min_df=2)\n",
    "\n",
    "print(\"\\nTop 20 BIGRAMAS (TEMAS, filtrado corporativo/CM):\")\n",
    "print(top_bi_topics.head(20).to_string(index=False))\n",
    "print(\"\\nTop 20 TRIGRAMAS (TEMAS, filtrado corporativo/CM):\")\n",
    "print(top_tri_topics.head(20).to_string(index=False))\n",
    "\n",
    "# --- RARE TERMS (para detectar alias) en df (global) ---\n",
    "brand_regex = r\"(sura|bol[ií]var|mapfre|allianz|colpatria|zurich|liberty|hdi|porvenir|suramericana|argos)\"\n",
    "token_lists = df[\"tokens_lemma\"].apply(\n",
    "    lambda x: x if isinstance(x, list)\n",
    "    else ([] if pd.isna(x) else (x.split() if isinstance(x, str) else []))\n",
    ")\n",
    "rare_terms = rare_terms_from_tokens(\n",
    "    token_lists.tolist(),\n",
    "    stopwords_set=sw_es,\n",
    "    min_len=3,\n",
    "    max_freq=3\n",
    ")\n",
    "looks_like_brand = rare_terms[rare_terms[\"term\"].str.contains(brand_regex, case=False, regex=True, na=False)]\n",
    "print(\"\\nRARE TERMS que parecen marcas/variantes (muestra 30):\")\n",
    "print(looks_like_brand.head(30).to_string(index=False))\n",
    "\n",
    "# --- FORMAS DE MENCIÓN (n-gramas con marca) sobre df (global) ---\n",
    "texts_blocked = df[\"text_blocked\"].fillna(\"\").astype(str).tolist()\n",
    "brand_bigrams  = ngram_counts_including_brands(texts_blocked, (2,2), min_df=1)\n",
    "brand_trigrams = ngram_counts_including_brands(texts_blocked, (3,3), min_df=1)\n",
    "\n",
    "print(\"\\nFORMAS DE MENCIÓN (BIGRAMAS con marca) - Top 30:\")\n",
    "print(brand_bigrams.head(30).to_string(index=False))\n",
    "print(\"\\nFORMAS DE MENCIÓN (TRIGRAMAS con marca) - Top 30:\")\n",
    "print(brand_trigrams.head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca45f1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https t</td>\n",
       "      <td>971</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t co</td>\n",
       "      <td>971</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positivacol colombiacompra</td>\n",
       "      <td>300</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>davidracero positivacol</td>\n",
       "      <td>293</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>colombiacompra wradiocolombia</td>\n",
       "      <td>290</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dcoronell davidracero</td>\n",
       "      <td>210</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aseguramiento salud</td>\n",
       "      <td>208</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sistema salud</td>\n",
       "      <td>208</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>patinaje velocidad</td>\n",
       "      <td>167</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>daviviendir segurosbolivar</td>\n",
       "      <td>165</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>davivienda bolivar</td>\n",
       "      <td>162</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>salud bolivar</td>\n",
       "      <td>131</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>modelo salud</td>\n",
       "      <td>127</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>supersalud positivacol</td>\n",
       "      <td>125</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>daviplata segurosbolivar</td>\n",
       "      <td>125</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>salud mental</td>\n",
       "      <td>122</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>segurosbolivar davivienda</td>\n",
       "      <td>122</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>modelo aseguramiento</td>\n",
       "      <td>120</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>millón pesos</td>\n",
       "      <td>117</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>senor supersalud</td>\n",
       "      <td>116</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>superintendencia financiero</td>\n",
       "      <td>111</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cemento argo</td>\n",
       "      <td>109</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sector asegurador</td>\n",
       "      <td>108</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>reforma salud</td>\n",
       "      <td>108</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>daviviendir bolivar</td>\n",
       "      <td>106</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ano pasado</td>\n",
       "      <td>104</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hacer parte</td>\n",
       "      <td>100</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>resultado oficial</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>preferencial grupo_sura</td>\n",
       "      <td>98</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>davivienda segurosbolivar</td>\n",
       "      <td>94</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>positivo compania</td>\n",
       "      <td>94</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>llevar cabo</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>24 7</td>\n",
       "      <td>91</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>seguridad salud</td>\n",
       "      <td>89</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>salud magisterio</td>\n",
       "      <td>85</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bolivar s</td>\n",
       "      <td>85</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>segurosbolivar daviplata</td>\n",
       "      <td>84</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>segurosbolivar sicsuper</td>\n",
       "      <td>82</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024 patinaje</td>\n",
       "      <td>82</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>msci colcap</td>\n",
       "      <td>81</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>negar atencion</td>\n",
       "      <td>81</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>compania bolivar</td>\n",
       "      <td>81</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>app surar</td>\n",
       "      <td>81</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>modelo sostenible</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>manera rapido</td>\n",
       "      <td>79</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>bolivar davivienda</td>\n",
       "      <td>79</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>caso manera</td>\n",
       "      <td>79</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>trav app</td>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>aqui dejar</td>\n",
       "      <td>78</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>sostenible aseguramiento</td>\n",
       "      <td>78</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ngram  freq   df\n",
       "0                         https t   971  469\n",
       "1                            t co   971  469\n",
       "2      positivacol colombiacompra   300  150\n",
       "3         davidracero positivacol   293  146\n",
       "4   colombiacompra wradiocolombia   290  145\n",
       "5           dcoronell davidracero   210  105\n",
       "6             aseguramiento salud   208  100\n",
       "7                   sistema salud   208  140\n",
       "8              patinaje velocidad   167   84\n",
       "9      daviviendir segurosbolivar   165   92\n",
       "10             davivienda bolivar   162  138\n",
       "11                  salud bolivar   131   81\n",
       "12                   modelo salud   127   94\n",
       "13         supersalud positivacol   125   65\n",
       "14       daviplata segurosbolivar   125   65\n",
       "15                   salud mental   122   66\n",
       "16      segurosbolivar davivienda   122   62\n",
       "17           modelo aseguramiento   120   48\n",
       "18                   millón pesos   117   97\n",
       "19               senor supersalud   116   59\n",
       "20    superintendencia financiero   111   68\n",
       "21                   cemento argo   109   68\n",
       "22              sector asegurador   108   79\n",
       "23                  reforma salud   108   66\n",
       "24            daviviendir bolivar   106   83\n",
       "25                     ano pasado   104   81\n",
       "26                    hacer parte   100   74\n",
       "27              resultado oficial   100   50\n",
       "28        preferencial grupo_sura    98   76\n",
       "29      davivienda segurosbolivar    94   53\n",
       "30              positivo compania    94   54\n",
       "31                    llevar cabo    93   70\n",
       "32                           24 7    91   47\n",
       "33                seguridad salud    89   55\n",
       "34               salud magisterio    85   61\n",
       "35                      bolivar s    85   68\n",
       "36       segurosbolivar daviplata    84   41\n",
       "37        segurosbolivar sicsuper    82   42\n",
       "38                  2024 patinaje    82   41\n",
       "39                    msci colcap    81   65\n",
       "40                 negar atencion    81   42\n",
       "41               compania bolivar    81   76\n",
       "42                      app surar    81   42\n",
       "43              modelo sostenible    80   40\n",
       "44                  manera rapido    79   41\n",
       "45             bolivar davivienda    79   62\n",
       "46                    caso manera    79   40\n",
       "47                       trav app    78   42\n",
       "48                     aqui dejar    78   39\n",
       "49       sostenible aseguramiento    78   39"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bi_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffbcdf",
   "metadata": {},
   "source": [
    "# Exploración de Kmeans\n",
    "- representar con TF-IDF (1–2),\n",
    "- generar temas iniciales sin etiquetas (clustering),\n",
    "- bautizarlos con nombres tentativos a partir de sus términos top,\n",
    "- cruzarlos con marcas para ver distribución,\n",
    "- exportar un dataset listo para revisión/etiquetado fino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "772578bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus final para KMeans: 9406 tweets (limpio de noticias y CM)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Filtrado adicional: excluir noticias corporativas y respuestas CM\n",
    "# ============================================================\n",
    "import re\n",
    "\n",
    "# --- FINANZAS / CORPORATIVO ---\n",
    "# Evita que noticias sobre utilidades o reportes financieros sesguen los clusters\n",
    "CORP_NEWS_PATTERNS |= {\n",
    "    \"billon\", \"utilidad\", \"utilidades\", \"colcap\", \"accion\", \"acciones\", \"preferencial\",\n",
    "    \"ranking\", \"trimestre\", \"semestre\", \"msci\", \"bolsa\", \"capitalizacion\", \"ingresos\",\n",
    "    \"resultado oficial\", \"valoracion\", \"dividendo\", \"emision\", \"empresa mas grande\"\n",
    "}\n",
    "\n",
    "# --- RESPUESTAS DE COMMUNITY MANAGER ---\n",
    "# Detecta mensajes genéricos sin contenido (para no agruparlos como “temas”)\n",
    "CM_TEMPLATES |= {\n",
    "    \"por interno\", \"escribenos por interno\", \"escríbenos por interno\",\n",
    "    \"escribenos al dm\", \"escríbenos al dm\", \"canales oficiales\", \"link en\",\n",
    "    \"aqui te dejamos el link\", \"dejanos tus datos\", \"dejar tus datos\",\n",
    "    \"mensaje directo\", \"favor escribanos\", \"contactanos por interno\"\n",
    "}\n",
    "\n",
    "# --- FUNCIONES DE MARCADO ---\n",
    "def is_corporate_news(text):\n",
    "    t = text.lower()\n",
    "    return any(pat in t for pat in CORP_NEWS_PATTERNS)\n",
    "\n",
    "def is_cm_template(text):\n",
    "    t = text.lower()\n",
    "    return any(pat in t for pat in CM_TEMPLATES)\n",
    "\n",
    "# --- Recalcular banderas (flags) y df_topics ---\n",
    "df[\"flag_corp_news\"] = df[\"text_clean_base\"].map(is_corporate_news)\n",
    "df[\"flag_cm_reply\"]  = df[\"text_clean_base\"].map(is_cm_template)\n",
    "\n",
    "# Excluir estos tweets para concentrarse en menciones con contenido real\n",
    "df_topics = df[~df[\"flag_corp_news\"] & ~df[\"flag_cm_reply\"]].copy()\n",
    "\n",
    "print(f\"Corpus final para KMeans: {len(df_topics)} tweets (limpio de noticias y CM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9590b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: docs=9406, vocab=9341\n",
      "Tamaños por cluster: [195, 1036, 5490, 546, 729, 500, 910]\n",
      "\n",
      "Top términos por cluster (primeras 15 features):\n",
      "\n",
      "Cluster 0: avenida, santander, calle, 15, calle 15, 2024, avenida santander, glorieta, edificio, cartagena, san, martin, pico, crespo, avenida san\n",
      "\n",
      "Cluster 1: s, ungrd, salud, tener, compania, soat, carrotanqu, contrato, general, guajira, magisterio, entidad, seguros_del_estado, gestion, proceso\n",
      "\n",
      "Cluster 2: ano, davidracero, wradiocolombia, millón, tener, dcoronell, banco, pais, davidracero wradiocolombia, asi, financiero, 2024, servicio, sector, social\n",
      "\n",
      "Cluster 3: hacer, pagar, hora, tener, empleo, vacante, salario, 2, servicio, buscar, comercial, 2 hora, hacer 2, millón, trabajar\n",
      "\n",
      "Cluster 4: llamar, decir, senor, hacer, seguir, servicio, nunca, dar, esperar, tener, cancelar, mes, cobrar, responder, dinero\n",
      "\n",
      "Cluster 5: eps, salud, aseguramiento, estatal, aseguramiento salud, sistema, sistema salud, modelo, retiro, reforma, modelo aseguramiento, usuario, millón, voluntario, retiro voluntario\n",
      "\n",
      "Cluster 6: dia, ir, laboral, arl, mejor, bienestar, salud, cada, trabajo, tener, seguridad, hacer, aqui, trabajador, alianza\n",
      "\n",
      "Ejemplos representativos por cluster (índices de df_topics):\n",
      "\n",
      "Cluster 0 -> 5 ejemplos\n",
      "  · id=15460 | brand= | text=Pico y placa en Cartagena del viernes 3 de enero de 2025 ...(Tubo Caribe). 4. Avenida Santander: Entre el Túnel de Crespo y la calle 15 (edi...\n",
      "  · id=2444 | brand= | text=Pico y placa en Cartagena del lunes 07 de octubre de 2024 ...municipio de Turbaco (Tubo Caribe). 4. Avenida Santander: Entre el Túnel de Cre...\n",
      "  · id=15146 | brand= | text=Pico y placa en Cartagena del viernes 24 de enero de 2025 ...municipio de Turbaco (Tubo Caribe). 4. Avenida Santander: Entre el Túnel de Cre...\n",
      "  · id=10262 | brand= | text=Pico y placa en Cartagena: así funcionará el miércoles 27 de agosto de 2025 ...Caribe). 4. Avenida Santander: entre el Túnel de Crespo y la ...\n",
      "  · id=2085 | brand= | text=Pico y placa en Cartagena del jueves 17 de octubre de 2024 ...Caribe). 4. Avenida Santander: Entre el Túnel de Crespo y la calle 15 (edifici...\n",
      "\n",
      "Cluster 1 -> 5 ejemplos\n",
      "  · id=2953 | brand=liberty seguros | text=Fraude en seguros: vehículos y salud encabezan los casos más frecuentes en Colombia ...Seguros; La Previsora S.A. Compañía de Seguros; Liber...\n",
      "  · id=9839 | brand=mapfre | text=SOAT 2024: ¿Dónde comprarlo con descuento? Aquí te contamos ...Liberty Seguros S.A. Mapfre Seguros Generales de Colombia S.A. Seguros Bolíva...\n",
      "  · id=4296 | brand=liberty seguros | text=De acuerdo con Fasecolda, estafa en compra del Soat afecta al año a 5.000 personas ...Equidad Seguros; La Previsora S.A. Compañía de Seguros...\n",
      "  · id=56 | brand= | text=Así puede obtener el Soat en 2025 y evitar ser estafado: estas son las empresas autorizadas ...Compañía Mundial de Seguros, S.A.Seguros Gene...\n",
      "  · id=2440 | brand=Fiduciaria La Previsora | text=¿Qué pasará con los carrotanques de la UNGRD inactivos en La Guajira? ...Procuraduría General de la Nación dio a conocer que citó de urgenci...\n",
      "\n",
      "Cluster 2 -> 5 ejemplos\n",
      "  · id=12638 | brand=Positiva Compañía de Seguros | text=@DCoronell @DavidRacero @PositivaCol @colombiacompra @WRadioColombia Y ni así despegarán los tibios @DCoronell @DavidRacero @PositivaCol @co...\n",
      "  · id=12678 | brand=Positiva Compañía de Seguros | text=@GuerreroSantos @DCoronell @DavidRacero @PositivaCol @colombiacompra @WRadioColombia ¿Ustedes no eran el cambio? @GuerreroSantos @DCoronell ...\n",
      "  · id=12565 | brand=Positiva Compañía de Seguros | text=@PatriotaLi42543 @DCoronell @DavidRacero @PositivaCol @colombiacompra @WRadioColombia Diría no. Ya lo dijeron. @PatriotaLi42543 @DCoronell @...\n",
      "  · id=12598 | brand=Positiva Compañía de Seguros | text=@DCoronell @DavidRacero @PositivaCol @colombiacompra @WRadioColombia Qué joyita! @DCoronell @DavidRacero @PositivaCol @colombiacompra @WRadi...\n",
      "  · id=12735 | brand=Positiva Compañía de Seguros | text=@carlosudea @DCoronell @DavidRacero @PositivaCol @colombiacompra @WRadioColombia Hasta éso @carlosudea @DCoronell @DavidRacero @PositivaCol ...\n",
      "\n",
      "Cluster 3 -> 5 ejemplos\n",
      "  · id=11117 | brand=seguros bolivar | text=Hagan la recompraaaa @GrupoBolivar @SegurosBolivar Hagan la recompraaaa @GrupoBolivar @SegurosBolivar...\n",
      "  · id=12375 | brand=seguros bolivar | text=@Misiafaustina @SegurosBolivar Eso se está haciendo. @Misiafaustina @SegurosBolivar Eso se está haciendo....\n",
      "  · id=8238 | brand=liberty seguros | text=El 88% de los colombianos prefieren las entidades financieras que apoyan las causas sociales y medioambientales ...el canal Más Chic hace 2 ...\n",
      "  · id=8236 | brand=liberty seguros | text=Colombia Living Luxury Properties te ofrece propiedades de lujo a tu alcance ...Hoteles Favoritos por el canal Más Chic hace 2 horas En esta...\n",
      "  · id=8243 | brand=liberty seguros | text=Día Mundial del Riñón, conciencia sobre la Enfermedad Renal ...canal Más Chic hace 2 horas En esta nueva edición del programa, Esteban Merce...\n",
      "\n",
      "Cluster 4 -> 5 ejemplos\n",
      "  · id=6466 | brand=seguros bolivar | text=Me paso lo mismo, nunca lo acepte y me lo cobran, luego lo cancele. Nunca lo hicieron y me siguieron cobrando. Uno llama Y lo dejan esperand...\n",
      "  · id=5454 | brand=seguros bolivar | text=Señor lo llamamos de Seguros Bolivar para... Señor lo llamamos de Seguros Bolivar para......\n",
      "  · id=4015 | brand=seguros bolivar | text=@SFCsupervisor , @SegurosBolivar  esta ROBANDO dinero directamente de la aplicación @DaviPlata  , yo nunca autoricé un servicio de asistenci...\n",
      "  · id=4049 | brand=seguros bolivar | text=@maurocastrolo Con ellos nunca contestan las lineas, se llama a Daviplata y dicen que no es su responsabilidad, se llama a @SegurosBolivar y...\n",
      "  · id=10494 | brand=seguros bolivar | text=@JpeOji @Davivienda @SegurosBolivar Dice q lo adquirí por medio de mi cuenta de @Davivienda  Y llamo al número q dice ahí a cancelar. Y no c...\n",
      "\n",
      "Cluster 5 -> 5 ejemplos\n",
      "  · id=3966 | brand=sura | text=EPS Salud Bolívar pidió su retiro voluntario del sistema nacional de salud ...EPS Salud Bolívar, adscrita a Seguros Bolívar, solicitó este m...\n",
      "  · id=7925 | brand=sura | text=@NoticiasCaracol Teniendo el gobierno la EPS más grande, Nueva EPS y aseguradoras Estatales como La Previsora...no ha tenido capacidad de de...\n",
      "  · id=3997 | brand=Fiduciaria La Previsora | text=@MarthaAlfonsoJ @EnfoqueTeve Rte. Martha, debería ser uno de los fundamentales de la reforma a la salud, la creación de modelos sostenibles ...\n",
      "  · id=5213 | brand= | text=Otra EPS pide retiro del sistema de salud ..., 5 de junio, la EPS Salud Bolívar pidió a la Superintendencia Nacional de Salud, tramitar su r...\n",
      "  · id=7673 | brand= | text=@vcalvot Mejor dedicar el esfuerzo al fortalecimiento de un sistema de aseguramiento integral en salud sostenible, entre las Estatales Nueva...\n",
      "\n",
      "Cluster 6 -> 5 ejemplos\n",
      "  · id=15185 | brand= | text=@Tuiran1319 @SFCsupervisor Hola, Charlie. Nos interesa mucho ayudarte. Hemos desarrollado una herramienta para atenderte de manera directa. ...\n",
      "  · id=15184 | brand= | text=@Tuiran1319 @SFCsupervisor Hola, Charlie. Nos interesa mucho ayudarte. Hemos desarrollado una herramienta para atenderte de manera directa. ...\n",
      "  · id=15183 | brand= | text=@Tuiran1319 Hola, Charlie. Nos interesa mucho ayudarte. Hemos desarrollado una herramienta para atenderte de manera directa. Aquí te dejamos...\n",
      "  · id=4960 | brand= | text=@Gonzo_Vasquez Hola, Gonzo. Nos interesa mucho ayudarte. Hemos desarrollado una herramienta para atenderte de manera directa. Aquí te dejamo...\n",
      "  · id=6876 | brand= | text=@VelandiaDeivy Hola, Deivy. Nos interesa mucho ayudarte. Hemos desarrollado una herramienta para atenderte de manera directa. Aquí te dejamo...\n",
      "\n",
      "Distribución global de temas (KMeans):\n",
      "           topic_name  n_docs\n",
      "Cobertura/Condiciones    5490\n",
      "     Atención/Trámite    1036\n",
      "          Otros/Mixto     910\n",
      "        Costos/Primas     729\n",
      "   Red/Autorizaciones     546\n",
      " Salud/EPS/Regulación     500\n",
      "     Pagos/Siniestros     195\n",
      "\n",
      "Distribución por marca y tema (primeras filas):\n",
      "               brand_primary            topic_name  n_docs\n",
      "                             Cobertura/Condiciones    2393\n",
      "                                  Atención/Trámite     492\n",
      "                                       Otros/Mixto     323\n",
      "                                Red/Autorizaciones     165\n",
      "                              Salud/EPS/Regulación     163\n",
      "                                  Pagos/Siniestros     147\n",
      "                                     Costos/Primas      50\n",
      "     Fiduciaria La Previsora Cobertura/Condiciones     382\n",
      "     Fiduciaria La Previsora      Atención/Trámite     318\n",
      "     Fiduciaria La Previsora  Salud/EPS/Regulación     100\n",
      "     Fiduciaria La Previsora           Otros/Mixto      90\n",
      "     Fiduciaria La Previsora    Red/Autorizaciones      31\n",
      "     Fiduciaria La Previsora         Costos/Primas      11\n",
      "Positiva Compañía de Seguros Cobertura/Condiciones     511\n",
      "Positiva Compañía de Seguros           Otros/Mixto     115\n",
      "Positiva Compañía de Seguros         Costos/Primas     106\n",
      "Positiva Compañía de Seguros  Salud/EPS/Regulación      41\n",
      "Positiva Compañía de Seguros    Red/Autorizaciones      32\n",
      "Positiva Compañía de Seguros      Pagos/Siniestros      28\n",
      "Positiva Compañía de Seguros      Atención/Trámite      12\n",
      "\n",
      "Exportado: corpus_con_temas_kmeans2.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Representación TF-IDF (1–2) + KMeans (temas)\n",
    "#  - Limpieza extra: exclusión de n-gramas/términos ruidosos\n",
    "# ============================================\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# -------- 0) Entradas / corpus tema-centrado --------\n",
    "# Usamos df_topics, creado antes (sin corporativo/CM). Si no existe, cae a df.\n",
    "if 'df_topics' not in globals():\n",
    "    df_topics = df[~df[\"flag_corp_news\"] & ~df[\"flag_cm_reply\"]].copy()\n",
    "\n",
    "texts = df_topics[\"text_topics\"].fillna(\"\").astype(str).tolist()\n",
    "doc_index = df_topics.index.to_list()  # para mapear de vuelta\n",
    "\n",
    "# -------- 1) Lista de exclusión (n-gramas y términos ruidosos) --------\n",
    "EXCLUDE_TERMS = [\n",
    "    # --- URLs y tokens genéricos ---\n",
    "    \"https\", \"t\", \"co\", \"https t\", \"t co\", \"24 7\",\n",
    "\n",
    "    # --- Marcas y entidades ---\n",
    "    \"segurosbolivar\", \"davivienda\", \"daviviendir\", \"bolivar\",\n",
    "    \"grupo_sura\", \"sura\", \"axa_colpatria\", \"axacolpatria\",\n",
    "    \"mapfre_co\", \"liberty\", \"positivacol\", \"positiva\",\n",
    "    \"previsora\", \"previsoro\", \"previsora_s\", \"previsora s.a.\",\n",
    "    \"hdi\", \"zurich\", \"colpatria\", \"fomag\", \"fasecolda\",\n",
    "    \"sfcsupervisor\", \"sicsuper\", \"supersalud\", \"superintendencia financiero\",\n",
    "    \"defensoriacol\", \"daviplata\", \"banco davivienda\",\n",
    "\n",
    "    # --- N-gramas compuestos de marcas ---\n",
    "    \"daviviendir segurosbolivar\", \"segurosbolivar davivienda\",\n",
    "    \"segurosbolivar daviplata\", \"daviplata segurosbolivar\",\n",
    "    \"davivienda bolivar\", \"bolivar davivienda\",\n",
    "    \"salud bolivar\", \"daviviendir bolivar\", \"compania bolivar\",\n",
    "    \"positivo compania\", \"positivacol colombiacompra\",\n",
    "    \"supersalud positivacol\", \"senor supersalud\",\n",
    "    \"cemento argo\", \"preferencial grupo_sura\",\n",
    "    \"app surar\", \"trav app\",\n",
    "\n",
    "    # --- Institucional / financiero ---\n",
    "    \"sector asegurador\", \"msci colcap\", \"millón pesos\", \"ano pasado\",\n",
    "    \"resultado oficial\", \"2024 patinaje\", \"modelo salud\",\n",
    "    \"modelo aseguramiento\", \"modelo sostenible\",\n",
    "    \"sostenible aseguramiento\", \"aseguramiento salud\",\n",
    "    \"seguridad salud\", \"sistema salud\", \"reforma salud\",\n",
    "    \"salud magisterio\", \"salud mental\",\n",
    "\n",
    "    # --- Expresiones genéricas vacías de contenido ---\n",
    "    \"hacer parte\", \"llevar cabo\", \"caso manera\",\n",
    "    \"aqui dejar\", \"manera rapido\", \"dejar link\"\n",
    "]\n",
    "\n",
    "# Preparamos regex para n-gramas (multi-palabra) y términos de 1 palabra\n",
    "EXCLUDE_MULTI = sorted([t for t in EXCLUDE_TERMS if \" \" in t], key=len, reverse=True)\n",
    "EXCLUDE_UNI   = {t for t in EXCLUDE_TERMS if \" \" not in t}\n",
    "\n",
    "def strip_excluded_ngrams(text: str) -> str:\n",
    "    \"\"\"Elimina n-gramas multi-palabra usando límites de palabra; colapsa espacios.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    t = f\" {text} \"\n",
    "    for ng in EXCLUDE_MULTI:\n",
    "        # \\b para límites, espacios convertidos a \\s+ para tolerar múltiples espacios\n",
    "        pat = re.compile(rf'(?i)(?<!\\w){re.escape(ng).replace(r\"\\ \", r\"\\s+\")}(?!\\w)')\n",
    "        t = pat.sub(\" \", t)\n",
    "    # limpia espacios\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# Aplicamos la limpieza de n-gramas ANTES de vectorizar\n",
    "texts_clean = [strip_excluded_ngrams(x) for x in texts]\n",
    "\n",
    "# -------- 2) TF-IDF (unigramas + bigramas) --------------\n",
    "# Stopwords: español NLTK + personalizadas + unigrams de exclusión\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "try:\n",
    "    _ = stopwords.words('spanish')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stop_es = set(stopwords.words('spanish')) | {\"https\", \"t\", \"co\", \"él\", \"la\", \"los\", \"las\"}\n",
    "# añadimos los UNIGRAMAS de la lista de exclusión a las stopwords\n",
    "stop_es = stop_es | EXCLUDE_UNI\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.6,\n",
    "    max_features=50000,\n",
    "    lowercase=False,\n",
    "    stop_words=sorted(list(stop_es)),  # sklearn requiere list o 'english'\n",
    "    token_pattern=r'(?u)\\b\\w+\\b'\n",
    ")\n",
    "\n",
    "X = tfidf.fit_transform(texts_clean)\n",
    "terms = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "print(f\"TF-IDF: docs={X.shape[0]}, vocab={X.shape[1]}\")\n",
    "\n",
    "# -------- 3) KMeans (elige k) ---------------------------\n",
    "# Sugerencia: 6–8 temas iniciales para este dominio\n",
    "K = 7\n",
    "km = MiniBatchKMeans(n_clusters=K, random_state=42, batch_size=2048, n_init='auto')\n",
    "labels = km.fit_predict(X)\n",
    "\n",
    "df_topics[\"topic_km\"] = labels\n",
    "\n",
    "# Reporte de tamaños por cluster\n",
    "sizes = np.bincount(labels, minlength=K)\n",
    "print(\"Tamaños por cluster:\", sizes.tolist())\n",
    "\n",
    "# -------- 4) Top términos por cluster -------------------\n",
    "def top_terms_per_cluster(X, labels, terms, topn=15):\n",
    "    centroids = km.cluster_centers_\n",
    "    tops = {}\n",
    "    for k in range(centroids.shape[0]):\n",
    "        idx = np.argsort(centroids[k])[::-1][:topn]\n",
    "        tops[k] = list(terms[idx])\n",
    "    return tops\n",
    "\n",
    "cluster_terms = top_terms_per_cluster(X, labels, terms, topn=15)\n",
    "\n",
    "print(\"\\nTop términos por cluster (primeras 15 features):\")\n",
    "for k, feats in cluster_terms.items():\n",
    "    print(f\"\\nCluster {k}: {', '.join(feats)}\")\n",
    "\n",
    "# -------- 5) Muestras representativas por cluster -------\n",
    "def representative_docs(X, labels, n=5):\n",
    "    reps = {}\n",
    "    centroids = km.cluster_centers_\n",
    "    dists = cosine_distances(X, centroids)\n",
    "    for k in range(centroids.shape[0]):\n",
    "        idx_k = np.where(labels == k)[0]\n",
    "        if len(idx_k) == 0:\n",
    "            reps[k] = []\n",
    "            continue\n",
    "        order = np.argsort(dists[idx_k, k])[:n]\n",
    "        reps[k] = idx_k[order]\n",
    "    return reps\n",
    "\n",
    "rep = representative_docs(X, labels, n=5)\n",
    "\n",
    "print(\"\\nEjemplos representativos por cluster (índices de df_topics):\")\n",
    "for k, idxs in rep.items():\n",
    "    print(f\"\\nCluster {k} -> {len(idxs)} ejemplos\")\n",
    "    for i in idxs:\n",
    "        row = df_topics.iloc[i]\n",
    "        txt_preview = str(row.get('text_raw', row.get('text_clean_base', '')))[:140].replace('\\n',' ')\n",
    "        print(f\"  · id={row.get('id', '')} | brand={row.get('brand_primary','')} | text={txt_preview}...\")\n",
    "\n",
    "# -------- 6) Tentativa de nombres de tema ----------------\n",
    "# Heurística inicial (ajústala leyendo 'cluster_terms' y muestras):\n",
    "topic_names = {\n",
    "    0: \"Pagos/Siniestros\",\n",
    "    1: \"Atención/Trámite\",\n",
    "    2: \"Cobertura/Condiciones\",\n",
    "    3: \"Red/Autorizaciones\",\n",
    "    4: \"Costos/Primas\",\n",
    "    5: \"Salud/EPS/Regulación\",\n",
    "    6: \"Otros/Mixto\"\n",
    "}\n",
    "df_topics[\"topic_name\"] = df_topics[\"topic_km\"].map(topic_names).fillna(\"Tema_sin_nombre\")\n",
    "\n",
    "# -------- 7) Cruce con marcas (distribución) -------------\n",
    "df_out = df.copy()\n",
    "df_out.loc[df_topics.index, \"topic_km\"] = df_topics[\"topic_km\"]\n",
    "df_out.loc[df_topics.index, \"topic_name\"] = df_topics[\"topic_name\"]\n",
    "\n",
    "tab_global = (\n",
    "    df_topics[\"topic_name\"].value_counts()\n",
    "    .rename_axis(\"topic_name\")\n",
    "    .reset_index(name=\"n_docs\")\n",
    ")\n",
    "tab_brand  = (\n",
    "    df_out.dropna(subset=[\"topic_name\"])\n",
    "         .groupby([\"brand_primary\",\"topic_name\"])\n",
    "         .size().reset_index(name=\"n_docs\")\n",
    "         .sort_values([\"brand_primary\",\"n_docs\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(\"\\nDistribución global de temas (KMeans):\")\n",
    "print(tab_global.to_string(index=False))\n",
    "\n",
    "print(\"\\nDistribución por marca y tema (primeras filas):\")\n",
    "print(tab_brand.head(20).to_string(index=False))\n",
    "\n",
    "# -------- 8) Export para revisión/etiquetado -------------\n",
    "cols_export = [\n",
    "    \"id\", \"Date\", \"brand_primary\", \"brand_list\",\n",
    "    \"text_raw\", \"text_clean_base\", \"text_topics\",\n",
    "    \"topic_km\", \"topic_name\"\n",
    "]\n",
    "export_topics_path = \"corpus_con_temas_kmeans2.csv\"\n",
    "df_out[cols_export].to_csv(export_topics_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nExportado: {export_topics_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2da889d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución global de temas finales:\n",
      "                                  topic_name  n_docs\n",
      "  Política/Economía/Medios/Contexto Nacional    5490\n",
      "SOAT/Fraudes/Carrotanques/Entidades Públicas    1036\n",
      " Respuestas Automáticas/Community Management     910\n",
      "   Quejas/Reclamos/Descuentos No Autorizados     729\n",
      "     Empleo/Salarios/Oportunidades Laborales     546\n",
      "       Salud/EPS/Retiro y Reforma al Sistema     500\n",
      "            Movilidad/Pico y Placa Cartagena     195\n",
      "\n",
      "Distribución por marca y tema (primeras filas):\n",
      "               brand_primary                                   topic_name  n_docs\n",
      "                               Política/Economía/Medios/Contexto Nacional    2393\n",
      "                             SOAT/Fraudes/Carrotanques/Entidades Públicas     492\n",
      "                              Respuestas Automáticas/Community Management     323\n",
      "                                  Empleo/Salarios/Oportunidades Laborales     165\n",
      "                                    Salud/EPS/Retiro y Reforma al Sistema     163\n",
      "                                         Movilidad/Pico y Placa Cartagena     147\n",
      "                                Quejas/Reclamos/Descuentos No Autorizados      50\n",
      "     Fiduciaria La Previsora   Política/Economía/Medios/Contexto Nacional     382\n",
      "     Fiduciaria La Previsora SOAT/Fraudes/Carrotanques/Entidades Públicas     318\n",
      "     Fiduciaria La Previsora        Salud/EPS/Retiro y Reforma al Sistema     100\n",
      "     Fiduciaria La Previsora  Respuestas Automáticas/Community Management      90\n",
      "     Fiduciaria La Previsora      Empleo/Salarios/Oportunidades Laborales      31\n",
      "     Fiduciaria La Previsora    Quejas/Reclamos/Descuentos No Autorizados      11\n",
      "Positiva Compañía de Seguros   Política/Economía/Medios/Contexto Nacional     511\n",
      "Positiva Compañía de Seguros  Respuestas Automáticas/Community Management     115\n",
      "Positiva Compañía de Seguros    Quejas/Reclamos/Descuentos No Autorizados     106\n",
      "Positiva Compañía de Seguros        Salud/EPS/Retiro y Reforma al Sistema      41\n",
      "Positiva Compañía de Seguros      Empleo/Salarios/Oportunidades Laborales      32\n",
      "Positiva Compañía de Seguros             Movilidad/Pico y Placa Cartagena      28\n",
      "Positiva Compañía de Seguros SOAT/Fraudes/Carrotanques/Entidades Públicas      12\n",
      "\n",
      "✅ Exportado correctamente: corpus_con_temas_final.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Nombramiento y guardado de clusters finales\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "\n",
    "# --- Asigna nombres interpretativos según análisis manual ---\n",
    "topic_names_final = {\n",
    "    0: \"Movilidad/Pico y Placa Cartagena\",\n",
    "    1: \"SOAT/Fraudes/Carrotanques/Entidades Públicas\",\n",
    "    2: \"Política/Economía/Medios/Contexto Nacional\",\n",
    "    3: \"Empleo/Salarios/Oportunidades Laborales\",\n",
    "    4: \"Quejas/Reclamos/Descuentos No Autorizados\",\n",
    "    5: \"Salud/EPS/Retiro y Reforma al Sistema\",\n",
    "    6: \"Respuestas Automáticas/Community Management\"\n",
    "}\n",
    "\n",
    "# Mapea los nombres en el DataFrame\n",
    "df_topics[\"topic_name\"] = df_topics[\"topic_km\"].map(topic_names_final).fillna(\"Tema_sin_nombre\")\n",
    "\n",
    "# Fusiona con df completo (para conservar marcas y texto original)\n",
    "df_out = df.copy()\n",
    "df_out.loc[df_topics.index, \"topic_km\"] = df_topics[\"topic_km\"]\n",
    "df_out.loc[df_topics.index, \"topic_name\"] = df_topics[\"topic_name\"]\n",
    "\n",
    "# Tablas resumen\n",
    "tab_global = (\n",
    "    df_topics[\"topic_name\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"topic_name\")\n",
    "    .reset_index(name=\"n_docs\")\n",
    ")\n",
    "tab_brand = (\n",
    "    df_out.dropna(subset=[\"topic_name\"])\n",
    "    .groupby([\"brand_primary\", \"topic_name\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"n_docs\")\n",
    "    .sort_values([\"brand_primary\", \"n_docs\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"\\nDistribución global de temas finales:\")\n",
    "print(tab_global.to_string(index=False))\n",
    "\n",
    "print(\"\\nDistribución por marca y tema (primeras filas):\")\n",
    "print(tab_brand.head(20).to_string(index=False))\n",
    "\n",
    "# --- Exportar resultados finales ---\n",
    "cols_export = [\n",
    "    \"id\", \"Date\", \"brand_primary\", \"brand_list\",\n",
    "    \"text_raw\", \"text_clean_base\", \"text_topics\",\n",
    "    \"topic_km\", \"topic_name\"\n",
    "]\n",
    "export_final_path = \"corpus_con_temas_final.csv\"\n",
    "df_out[cols_export].to_csv(export_final_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n✅ Exportado correctamente: {export_final_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a340b9",
   "metadata": {},
   "source": [
    "# Clasificacion Supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd0d8cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus cargado: (15469, 9)\n",
      "Textos válidos para entrenamiento: 9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 macro (test): 0.923\n",
      "\n",
      "Reporte de clasificación (test):\n",
      "                                              precision    recall  f1-score   support\n",
      "\n",
      "     Empleo/Salarios/Oportunidades Laborales      0.805     0.872     0.837       109\n",
      "            Movilidad/Pico y Placa Cartagena      1.000     1.000     1.000        39\n",
      "  Política/Economía/Medios/Contexto Nacional      0.981     0.923     0.951      1098\n",
      "   Quejas/Reclamos/Descuentos No Autorizados      0.824     0.959     0.886       146\n",
      " Respuestas Automáticas/Community Management      0.878     0.951     0.913       182\n",
      "SOAT/Fraudes/Carrotanques/Entidades Públicas      0.886     0.937     0.911       207\n",
      "       Salud/EPS/Retiro y Reforma al Sistema      0.942     0.980     0.961       100\n",
      "\n",
      "                                    accuracy                          0.932      1881\n",
      "                                   macro avg      0.902     0.946     0.923      1881\n",
      "                                weighted avg      0.936     0.932     0.933      1881\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      "                                                   pred:Empleo/Salarios/Oportunidades Laborales  pred:Movilidad/Pico y Placa Cartagena  pred:Política/Economía/Medios/Contexto Nacional  pred:Quejas/Reclamos/Descuentos No Autorizados  pred:Respuestas Automáticas/Community Management  pred:SOAT/Fraudes/Carrotanques/Entidades Públicas  pred:Salud/EPS/Retiro y Reforma al Sistema\n",
      "true:Empleo/Salarios/Oportunidades Laborales                                                 95                                      0                                                3                                               8                                                 0                                                  2                                           1\n",
      "true:Movilidad/Pico y Placa Cartagena                                                         0                                     39                                                0                                               0                                                 0                                                  0                                           0\n",
      "true:Política/Economía/Medios/Contexto Nacional                                              17                                      0                                             1014                                              20                                                23                                                 21                                           3\n",
      "true:Quejas/Reclamos/Descuentos No Autorizados                                                3                                      0                                                1                                             140                                                 1                                                  1                                           0\n",
      "true:Respuestas Automáticas/Community Management                                              1                                      0                                                6                                               1                                               173                                                  0                                           1\n",
      "true:SOAT/Fraudes/Carrotanques/Entidades Públicas                                             2                                      0                                                9                                               1                                                 0                                                194                                           1\n",
      "true:Salud/EPS/Retiro y Reforma al Sistema                                                    0                                      0                                                1                                               0                                                 0                                                  1                                          98\n",
      "\n",
      "Top 25 términos por tema:\n",
      "\n",
      "Empleo/Salarios/Oportunidades Laborales: pagar, hacer, hora, empleo, vacante, salario, buscar, comercial, asesor, aplicar, trabajar, pagina, bogota, tener, disponible, oferta, hacer ano, dano, 2, tres hora, ano, sueldo, ciudad, comercial bolivar, 2 hora\n",
      "\n",
      "Movilidad/Pico y Placa Cartagena: avenida, calle, santander, 2024, 15, seleccion, calle 15, martin, abril 25, 25 2024, patinaje velocidad, velocidad, avenida santander, patinaje, seleccion positivo, compania 2024, san, abril, 25, clasificado seleccion, team positivacol, clasificado, positivo compania, edificio, san martin\n",
      "\n",
      "Política/Economía/Medios/Contexto Nacional: sector, wradiocolombia, mayor, davidracero, trav, digital, joven, grupo_sura, anunciar, pensional, positivacol colombiacompra, mercado, auto, argo, davidracero positivacol, colombiacompra wradiocolombia, colombiacompra, dcoronell, gerente, allianz_colombia, proyecto, vivienda, sostenibilidad, viaje, lider\n",
      "\n",
      "Quejas/Reclamos/Descuentos No Autorizados: llamar, decir, esperar, seguir, segurosbolivar, senor, dar, cobrar, nunca, mes, servicio, contestar, responder, favor, daviplata, pasar, cancelar, sfcsupervisor, q, dinero, asistencia, descontar, respuesta, linea, numero\n",
      "\n",
      "Respuestas Automáticas/Community Management: dia, ir, arl, laboral, mejor, bienestar, cada, trabajo, seguridad, prevencion, trabajador, mujer, aqui, cambio, vez, ambiente, herramienta, proteccion, seguridad salud, cuidar, salud trabajo, cada vez, programa, enfermedad, hoy\n",
      "\n",
      "SOAT/Fraudes/Carrotanques/Entidades Públicas: s, ungrd, soat, contrato, compania, salud, tener, magisterio, carrotanqu, guajira, previsora_s, entidad, seguros_del_estado, maestro, general, salud magisterio, fiduprevisora, vehiculo, gestion, previsoro, modelo salud, fecode, carrillo, servicio salud, proceso\n",
      "\n",
      "Salud/EPS/Retiro y Reforma al Sistema: eps, salud, sistema, estatal, sistema salud, reforma, modelo, aseguramiento, reforma salud, grupo_sura, aseguramiento salud, modelo aseguramiento, petro, bluradioco, retiro, orientacion psicologico, positivamente lado, millón, ep, psicologico, positivamente, 24 7, culpa, petrogustavo, ahora\n",
      "\n",
      "F1 macro por marca (test) - top 15:\n",
      "                              n_test  f1_macro\n",
      "brand_primary                                 \n",
      "seguros bolivar                385.0  0.897935\n",
      "Fiduciaria La Previsora        181.0  0.842194\n",
      "Positiva Compañía de Seguros   167.0  0.953787\n",
      "sura                           146.0  0.944393\n",
      "mapfre                         111.0  0.888476\n",
      "axa colpatria                   55.0  0.652866\n",
      "allianz                         38.0  0.973696\n",
      "liberty seguros                 28.0  0.615000\n",
      "hdi seguros                     10.0  1.000000\n",
      "zurich                           2.0  1.000000\n",
      "porvenir                         1.0  1.000000\n",
      "\n",
      "Exportados:\n",
      "- pred_supervisado_temas_test.csv\n",
      "- errores_supervisado_temas_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_13576\\2547097012.py:96: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CLASIFICADOR SUPERVISADO DE TEMAS (desde corpus_con_temas_final.csv)\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# -------- 1) Carga del corpus final -----------\n",
    "df = pd.read_csv(\"corpus_con_temas_final.csv\")\n",
    "print(\"Corpus cargado:\", df.shape)\n",
    "\n",
    "# Verifica que existan las columnas esperadas\n",
    "assert all(col in df.columns for col in [\"text_topics\", \"topic_name\"]), \\\n",
    "    \"Faltan columnas necesarias ('text_topics', 'topic_name') en el CSV.\"\n",
    "\n",
    "# Filtra filas válidas\n",
    "df_sup = df.dropna(subset=[\"text_topics\", \"topic_name\"]).copy()\n",
    "print(\"Textos válidos para entrenamiento:\", len(df_sup))\n",
    "\n",
    "# -------- 2) Variables X / y -----------\n",
    "X_text = df_sup[\"text_topics\"].astype(str)\n",
    "y = df_sup[\"topic_name\"].astype(str)\n",
    "\n",
    "# -------- 3) Split entrenamiento / prueba -----------\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X_text, y, df_sup.index, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------- 4) Vectorización TF-IDF -----------\n",
    "sw_es = set(stopwords.words('spanish'))\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5,\n",
    "    max_df=0.6,\n",
    "    max_features=40000,\n",
    "    lowercase=False,\n",
    "    token_pattern=r'(?u)\\b\\w+\\b',\n",
    "    stop_words=list(sw_es)\n",
    ")\n",
    "Xtr = tfidf.fit_transform(X_train)\n",
    "Xte = tfidf.transform(X_test)\n",
    "\n",
    "# -------- 5) Modelo supervisado -----------\n",
    "clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=None,\n",
    "    solver=\"lbfgs\",\n",
    "    multi_class=\"auto\"\n",
    ")\n",
    "clf.fit(Xtr, y_train)\n",
    "\n",
    "# -------- 6) Evaluación -----------\n",
    "y_pred = clf.predict(Xte)\n",
    "print(\"\\nF1 macro (test):\", round(f1_score(y_test, y_pred, average=\"macro\"), 3))\n",
    "print(\"\\nReporte de clasificación (test):\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=sorted(y.unique()))\n",
    "cm_df = pd.DataFrame(cm, index=[f\"true:{c}\" for c in sorted(y.unique())],\n",
    "                        columns=[f\"pred:{c}\" for c in sorted(y.unique())])\n",
    "print(\"\\nMatriz de confusión:\")\n",
    "print(cm_df.to_string())\n",
    "\n",
    "# -------- 7) Palabras más representativas por tema -----------\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "classes = clf.classes_\n",
    "coefs = clf.coef_\n",
    "\n",
    "def top_features_for_class(k=25):\n",
    "    out = {}\n",
    "    for i, c in enumerate(classes):\n",
    "        idx = np.argsort(coefs[i])[::-1][:k]\n",
    "        out[c] = feature_names[idx]\n",
    "    return out\n",
    "\n",
    "tops = top_features_for_class(25)\n",
    "print(\"\\nTop 25 términos por tema:\")\n",
    "for c, feats in tops.items():\n",
    "    print(f\"\\n{c}: \" + \", \".join(feats))\n",
    "\n",
    "# -------- 8) Evaluación por marca -----------\n",
    "test_df = df_sup.loc[idx_test].copy()\n",
    "test_df[\"y_true\"] = y_test.values\n",
    "test_df[\"y_pred\"] = y_pred\n",
    "\n",
    "if \"brand_primary\" in df_sup.columns:\n",
    "    brand_eval = (\n",
    "        test_df.groupby(\"brand_primary\")\n",
    "               .apply(lambda g: pd.Series({\n",
    "                   \"n_test\": len(g),\n",
    "                   \"f1_macro\": f1_score(g[\"y_true\"], g[\"y_pred\"], average=\"macro\")\n",
    "               }))\n",
    "               .sort_values(\"n_test\", ascending=False)\n",
    "    )\n",
    "    print(\"\\nF1 macro por marca (test) - top 15:\")\n",
    "    print(brand_eval.head(15).to_string())\n",
    "\n",
    "# -------- 9) Export resultados -----------\n",
    "pred_path = \"pred_supervisado_temas_test.csv\"\n",
    "err_path  = \"errores_supervisado_temas_test.csv\"\n",
    "test_df.to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "test_df[test_df[\"y_true\"] != test_df[\"y_pred\"]].to_csv(err_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nExportados:\\n- {pred_path}\\n- {err_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ced29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74c0d306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cluster 0 (Cobertura/Condiciones) | n=237 ===\n",
      "- [mapfre] MAPFRE CRECIÓ 9% Y CONSIGUIÓ UN BENEFICIO DE 24% EN LATINOAMÉRICA DURANTE 2023 Grupo.  A nivel global, MAPFRE obtuvo un resultado neto de 692 millones de euros, lo que representa ...\n",
      "- [fiduciaria previsora] La Previsora reafirma su liderazgo en el sector asegurador del país gracias a sus múltiples distinciones ...reconocimiento a una vida de trabajo por la protección en el país. En un...\n",
      "- [] Directv lanza seguro todo riesgo para autos en Colombia: ¿qué beneficios tiene? ...Axa Colpatria. El nuevo seguro está diseñado para ofrecer a los conductores colombianos una alter...\n",
      "- [] Seis acreedores de la cadena de almacenes de perfumerías y farmacéuticos Fedco denuncian que les incumplió pagos del acuerdo que suma $273.504 millones ...noventa (90) días, por va...\n",
      "- [mapfre] Conozca las coberturas de seguros para las empresas que ofrecen fiestas navideñas ...; los accidentes personales y el seguro del personal. En específico, en el seguro de responsabi...\n",
      "\n",
      "=== Cluster 1 (Atención/Trámite) | n=320 ===\n",
      "- [] @adriana87745913 @ClaroColombia @MovistarCo @SegurosBolivar Hola, @adriana87745913. Para nosotros es muy importante tu tranquilidad, te invitamos a que nos envíes un mensaje por pr...\n",
      "- [seguros bolivar] Seguros Bolívar inauguró oficialmente la cuarta sede en el Centro Comercial Metrópolis, en Bogotá, donde se espera atender a 42.000 personas por año sumándose así a las 134.516 con...\n",
      "- [] @joel_mfc @SegurosBolivar @DaviPlata @Ministerio_TIC @joel_mfc 2/2 en el siguiente enlace: https://t.co/2nhmw5xsIH radica tu queja. Por otra parte, de presentarse alguna afectación...\n",
      "- [fiduciaria previsora] Procuraduría cita de urgencia a UNGRD y a las aseguradoras por carrotanques varados ...JUSTICIA La Procuraduría citó de urgencia el viernes 04 de octubre a la Unidad Nacional de Ge...\n",
      "- [] “Solicitar asistencia de su seguro nunca fue tan fácil” \n",
      "dice la grabación de @SegurosBolivar  cada 20 segundos mientras llevas un jurgo de tiempo esperando que un asesor te atiend...\n",
      "\n",
      "=== Cluster 2 (Pagos/Siniestros) | n=833 ===\n",
      "- [] @PositivaCol trabajen!! Esa plataforma no funciona!!! Necesito afiliar empleados y me es imposible!! Pero si están llegando los cobros 😠😠😠 @PositivaCol trabajen!! Esa plataforma no...\n",
      "- [] @MAPFRE_CO El tipo de césped, la humedad y el estado del suelo son factores claves. Un campo en malas condiciones, con áreas húmedas o secas, puede alterar la trayectoria del balón...\n",
      "- [] @SegurosBolivar Aunque me dieron respuesta no me dan solución. Me hacen descuentos pero aclaran que no es Seguros si no \"Servicios\" Bolivar. Que para mí viene siendo lo mismo, si s...\n",
      "- [axa colpatria] @AXACOLPATRIA Los número telefónicos de la clínica Vip no sirven las llamadas no entran, los correos electrónicos no los contestan. Quien responde en axacolpatria ? @AXACOLPATRIA L...\n",
      "- [] @DaviPlata @JersonAcevedo8 @SegurosBolivar Ya los contacte y no solucionan el problema devuelvan la plata abusivos @DaviPlata @JersonAcevedo8 @SegurosBolivar Ya los contacte y no s...\n",
      "\n",
      "=== Cluster 3 (Atención/Trámite) | n=187 ===\n",
      "- [] El bienestar mental es clave para crecer juntos. Hoy realizamos el taller de Conciencia y Autocuidado de la Salud Mental con @SegurosBolivar, aprendiendo a cuidar de nosotros y de ...\n",
      "- [fiduciaria previsora] Nuevo modelo de salud para docentes: Fecode anuncia cambios importantes a partir del 1 de mayo ...Fecode ha asegurado que la Fiduciaria La Previsora continuará gestionando los recu...\n",
      "- [fiduciaria previsora] Continúan plantones frente a Fiduprevisora ante fallas en servicios de salud al magisterio COMPARTIR EN: La Presidenta de ADEA, Marinelda Salas, advirtió que continuarán los planto...\n",
      "- [] Fundación Santa Fe suspende atención médica a maestros desde el 31 de julio ...Magisterio (FOMAG) a partir del próximo 31 de julio. La decisión responde a la terminación del contra...\n",
      "- [sura] 1 de cada 3 personas con ansiedad o depresión falta al trabajo: Seguros SURA alerta sobre el impacto de la salud mental en la productividad empresarial Durante 2024, la línea de at...\n",
      "\n",
      "=== Cluster 4 (Atención/Trámite) | n=62 ===\n",
      "- [liberty seguros] Estas serán las tarifas del Soat para el 2024 ...para ofrecerlo: Aseguradora Solidaria de Colombia Entidad Cooperativa Axa Colpatria Seguros S.A. Seguros Mundial La Equidad Seguros...\n",
      "- [seguros bolivar] Cuánto vale el SOAT para una moto 125 cc: Valor actualizado 2025 y cómo adquirirlo ..., Seguros Mundial, La Previsora S.A. Compañía de Seguros, Seguros Bolívar S.A., Seguros Genera...\n",
      "- [axa colpatria] Tarifas Soat 2025: listado completo de precios por categoría ...Obligatorio de Accidentes de Tránsito (Soat) son: Aseguradora Solidaria de ColombiaAxa Colpatria Seguros S.A.Seguros...\n",
      "- [sura] SOAT: 5 recomendaciones para comprarlo por Internet sin caer en un fraude ...Colombia Entidad Cooperativa.Axa Colpatria Seguros S.A.Seguros Mundial.La Equidad.La Previsora.Liberty ...\n",
      "- [liberty seguros] ¿Cuáles son las multas por no tener el Soat vigente en 2023? ...aseguradoras autorizadas para expedirlo son: Aseguradora Solidaria de Colombia Entidad Cooperativa Axa Colpatria Seg...\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# Mostrar ejemplos de tweets por cluster (servicio)\n",
    "# ================================================\n",
    "\n",
    "def print_examples(df, cluster_col=\"topic_km\", text_col=\"text_raw\", brand_col=\"brand_primary\", n=5):\n",
    "    clusters = df[cluster_col].unique()\n",
    "    for cl in sorted(clusters):\n",
    "        subset = df[df[cluster_col] == cl]\n",
    "        print(f\"\\n=== Cluster {cl} ({subset['topic_name'].iloc[0]}) | n={len(subset)} ===\")\n",
    "        sample = subset.sample(min(n, len(subset)), random_state=42)\n",
    "        for _, row in sample.iterrows():\n",
    "            print(f\"- [{row[brand_col]}] {row[text_col][:180]}...\")\n",
    "\n",
    "# Llamar a la función (ejemplos de 5 por cluster)\n",
    "print_examples(df_service, cluster_col=\"topic_km\", text_col=\"text_raw\", brand_col=\"brand_primary\", n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4831cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs servicio (nuevo filtro): 198  / de 9036\n",
      "TF-IDF servicio: docs=198, vocab=214\n",
      "K=4 -> tamaños por cluster: [39, 61, 51, 47]\n",
      "K=5 -> tamaños por cluster: [32, 65, 49, 43, 9]\n",
      "\n",
      "Cluster 0 tops:\n",
      "dejar, reparacion, parte, bogota, seguir, decir, solo, avanzar, linea_de_atencion, segun, 01, sistema, grupo_sura, cancelar, preparacion, deber, poner, salud, informacion, proceso, medio, comunicar, asistencia, eps, ano\n",
      "\n",
      "Cluster 1 tops:\n",
      "tener, siniestro, soat, responder, pagar, vehiculo, reparacion, correo, acuerdo, dia, dar, suarez, buen, hacer, documento, dato, llevar, 2024, s, radicado, saber, llamar, conductor, respuesta, pago\n",
      "\n",
      "Cluster 2 tops:\n",
      "autorizacion, descontar, hacer, dinero, pasar, descuento, llamar, robar, cuenta, servicio, decir, primero, s, mismo, hdi, positiva, financiero, bolivar, persona, ladrón, autorizar, 2, hora, solicitar, tratar\n",
      "\n",
      "Cluster 3 tops:\n",
      "siniestro, bolivar, servicio, axa_colpatria, asi, clave, atencion, pagar, compania, eps, medico, scotiabank, digital, dano, scotiabank colpatrio, producto, afectar, colpatrio, informar, gestion, accidente, salud, analisis, soat, manera\n",
      "\n",
      "Cluster 4 tops:\n",
      "tener, liberty, country, country manager, rodriguez, rodriguez country, cesar, cesar rodriguez, necesitar tener, manager liberty, senalar cesar, manager, siniestro necesitar, respaldo senalar, tener respaldo, respaldo, tener siniestro, senalar, desprotegido momento, enterar, enterar estir, soler enterar, desprotegido, estir desprotegido, sino soler\n",
      "\n",
      "Nombre por cluster:\n",
      "0 -> Pagos/Siniestros\n",
      "1 -> Atención/PQRS\n",
      "2 -> Atención/Trámite\n",
      "3 -> Pagos/Siniestros#3\n",
      "4 -> Pagos/Siniestros#4\n",
      "\n",
      "F1 macro (test): 0.889\n",
      "\n",
      "Reporte de clasificación (test):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Atención/PQRS      1.000     0.923     0.960        13\n",
      "  Atención/Trámite      1.000     1.000     1.000        10\n",
      "  Pagos/Siniestros      1.000     0.500     0.667         6\n",
      "Pagos/Siniestros#3      0.692     1.000     0.818         9\n",
      "Pagos/Siniestros#4      1.000     1.000     1.000         2\n",
      "\n",
      "          accuracy                          0.900        40\n",
      "         macro avg      0.938     0.885     0.889        40\n",
      "      weighted avg      0.931     0.900     0.896        40\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      "                         pred:Atención/PQRS  pred:Atención/Trámite  pred:Pagos/Siniestros  pred:Pagos/Siniestros#3  pred:Pagos/Siniestros#4\n",
      "true:Atención/PQRS                       12                      0                      0                        1                        0\n",
      "true:Atención/Trámite                     0                     10                      0                        0                        0\n",
      "true:Pagos/Siniestros                     0                      0                      3                        3                        0\n",
      "true:Pagos/Siniestros#3                   0                      0                      0                        9                        0\n",
      "true:Pagos/Siniestros#4                   0                      0                      0                        0                        2\n",
      "\n",
      "Exportados: pred_supervisado_temas_test.csv, errores_supervisado_temas_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) Filtro \"Servicio\" reforzado (semillas + señales de queja)\n",
    "#    y exclusión de corporativo/deporte/SOAT informativo\n",
    "# ============================================================\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "POSITIVE_SEEDS = re.compile(\n",
    "    r\"(siniestro|indemnizaci[oó]n|reembolso|radicado|reparaci[oó]n|peritaci[oó]n|ajustador|\"\n",
    "    r\"autorizaci[oó]n|red_m[eé]dica|cobertura|exclusi[oó]n|preexistenc|deducible|cl[aá]usula|\"\n",
    "    r\"condici[oó]n|prima|soat|pqrs|atenci[oó]n_al_cliente|l[ií]nea_de_atenci[oó]n)\",\n",
    "    re.I\n",
    ")\n",
    "COMPLAINT = re.compile(\n",
    "    r\"(no\\s+(respon|contes|solucion|pagan)|llevo\\s+\\d+\\s+(d[ií]as|meses)|\"\n",
    "    r\"por favor|ayuda|necesito|reclamo|queja|radicado|esperando|llamar|\"\n",
    "    r\"no han pagado|devoluci[oó]n|ticket|formulario|pqrs)\",\n",
    "    re.I\n",
    ")\n",
    "NOT_SERVICE = re.compile(\n",
    "    r\"(colcap|bill[oó]n|dividendo|resultado(?:s)?\\s+oficial|beneficio|\"\n",
    "    r\"inaugur|ranking|trimestre|semestre|msci|bolsa|utilidad(?:es)?|\"\n",
    "    r\"convocatoria|patinaje|selecci[oó]n|campeonato|asamblea|consejo directivo)\",\n",
    "    re.I\n",
    ")\n",
    "SOAT_INFO = re.compile(r\"(tarifa|valor|listado|c[oó]mo\\s+adquirir|recomendacione?s)\", re.I)\n",
    "\n",
    "def looks_like_service(text: str) -> bool:\n",
    "    t = text or \"\"\n",
    "    if NOT_SERVICE.search(t):  # descarta corporativo/deporte/finanzas\n",
    "        return False\n",
    "    if \"soat\" in t.lower() and SOAT_INFO.search(t) and not COMPLAINT.search(t):\n",
    "        return False  # SOAT informativo sin queja/pedido\n",
    "    # semillas + (queja/pedido) O semillas fuertes\n",
    "    has_seed = bool(POSITIVE_SEEDS.search(t))\n",
    "    has_complaint = bool(COMPLAINT.search(t))\n",
    "    strong = re.search(r\"(siniestro|autorizaci[oó]n|radicado|reparaci[oó]n|red_m[eé]dica)\", t, re.I)\n",
    "    return bool(has_seed and (has_complaint or strong))\n",
    "\n",
    "# Asegura df_topics (tema-centrado sin corporativo/CM); si no existe, créalo\n",
    "if 'df_topics' not in globals():\n",
    "    df_topics = df[~df[\"flag_corp_news\"] & ~df[\"flag_cm_reply\"]].copy()\n",
    "\n",
    "df_service = df_topics[df_topics[\"text_topics\"].map(looks_like_service)].copy()\n",
    "print(\"Docs servicio (nuevo filtro):\", len(df_service), \" / de\", len(df_topics))\n",
    "\n",
    "# ============================================================\n",
    "# 2) TF-IDF (1–2) + KMeans en servicio\n",
    "# ============================================================\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "sw_es = set(stopwords.words('spanish'))\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5,\n",
    "    max_df=0.6,\n",
    "    max_features=40000,\n",
    "    lowercase=False,\n",
    "    token_pattern=r'(?u)\\b\\w+\\b',\n",
    "    stop_words=list(sw_es)\n",
    ")\n",
    "texts = df_service[\"text_topics\"].fillna(\"\").astype(str).tolist()\n",
    "X = tfidf.fit_transform(texts)\n",
    "terms = np.array(tfidf.get_feature_names_out())\n",
    "print(f\"TF-IDF servicio: docs={X.shape[0]}, vocab={X.shape[1]}\")\n",
    "\n",
    "# prueba K (elige el que te deje 3–5 grupos razonables)\n",
    "for K in [4,5]:\n",
    "    km_tmp = MiniBatchKMeans(n_clusters=K, random_state=42, batch_size=2048, n_init='auto')\n",
    "    lbl_tmp = km_tmp.fit_predict(X)\n",
    "    sizes = pd.Series(lbl_tmp).value_counts().sort_index().tolist()\n",
    "    print(f\"K={K} -> tamaños por cluster:\", sizes)\n",
    "\n",
    "K = 5\n",
    "km = MiniBatchKMeans(n_clusters=K, random_state=42, batch_size=2048, n_init='auto')\n",
    "labels = km.fit_predict(X)\n",
    "df_service[\"topic_km\"] = labels\n",
    "\n",
    "# Top términos por cluster (para inspección)\n",
    "centroids = km.cluster_centers_\n",
    "cluster_top_terms = {}\n",
    "for k in range(K):\n",
    "    top_idx = np.argsort(centroids[k])[::-1][:25]\n",
    "    tops = terms[top_idx]\n",
    "    cluster_top_terms[k] = \", \".join(tops)\n",
    "    print(f\"\\nCluster {k} tops:\\n{cluster_top_terms[k]}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Bautizo SIN duplicados (reglas + subreglas)\n",
    "# ============================================================\n",
    "RULES_PRIMARY = [\n",
    "    (\"Pagos/Siniestros\",  r\"(siniestro|indemnizaci[oó]n|reembolso|pago_de_siniestro|radicado|peritaci[oó]n|ajustador|reparaci[oó]n)\"),\n",
    "    (\"Atención/Trámite\",  r\"(atenci[oó]n_al_cliente|l[ií]nea_de_atenci[oó]n|tiempo_de_respuesta|pqrs|queja|reclamo|responder|esperar|llamar|no\\s+(respon|contes|solucion))\"),\n",
    "    (\"Cobertura/Condiciones\", r\"(cobertura|exclusi[oó]n|preexistenc|deducible|cl[aá]usula|condici[oó]n)\"),\n",
    "    (\"Red/Autorizaciones\", r\"(red_m[eé]dica|autorizaci[oó]n_de_servicio|autorizaci[oó]n|cl[ií]nica|prestador|cita|remisi[oó]n|taller)\"),\n",
    "    (\"Costos/Primas/SOAT\", r\"(prima|incremento_de_prima|facturaci[oó]n|cobro|d[eé]bito|soat)\")\n",
    "]\n",
    "RULES_SECONDARY = [\n",
    "    (\"Atención/PQRS\", r\"(pqrs|radicado|formulario|ticket|canal)\"),\n",
    "    (\"Atención/Respuesta\", r\"(esperar|no\\s+(respon|contes)|l[ií]nea_de_atenci[oó]n|llamar)\"),\n",
    "    (\"Pagos/Reparación\", r\"(reparaci[oó]n|taller|peritaje)\"),\n",
    "    (\"Cobertura/Preexistencias\", r\"(preexistenc)\"),\n",
    "]\n",
    "\n",
    "def name_by_rules(text):\n",
    "    # primaria\n",
    "    for name, rx in RULES_PRIMARY:\n",
    "        if re.search(rx, text, re.I):\n",
    "            return name\n",
    "    # secundaria\n",
    "    for name, rx in RULES_SECONDARY:\n",
    "        if re.search(rx, text, re.I):\n",
    "            return name\n",
    "    return \"Tema_mixto\"\n",
    "\n",
    "# asigna nombres; si hay duplicados, desambigua con secundarias / id\n",
    "names = {}\n",
    "used = set()\n",
    "for k in range(K):\n",
    "    base = name_by_rules(cluster_top_terms[k])\n",
    "    if base in used:\n",
    "        # intenta secundaria para diferenciar\n",
    "        sec = None\n",
    "        for nm, rx in RULES_SECONDARY:\n",
    "            if re.search(rx, cluster_top_terms[k], re.I) and nm not in used:\n",
    "                sec = nm; break\n",
    "        name = sec or f\"{base}#{k}\"\n",
    "    else:\n",
    "        name = base\n",
    "    names[k] = name\n",
    "    used.add(name)\n",
    "\n",
    "print(\"\\nNombre por cluster:\")\n",
    "for k, n in names.items():\n",
    "    print(k, \"->\", n)\n",
    "\n",
    "df_service[\"topic_name\"] = df_service[\"topic_km\"].map(names)\n",
    "\n",
    "# ============================================================\n",
    "# 4) Supervisado (TF-IDF + LogReg) + matriz de confusión\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "df_sup = df_service.dropna(subset=[\"text_topics\",\"topic_name\"]).copy()\n",
    "X_text = df_sup[\"text_topics\"].astype(str)\n",
    "y      = df_sup[\"topic_name\"].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X_text, y, df_sup.index, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "tfidf_sup = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5, max_df=0.6, max_features=40000,\n",
    "    lowercase=False, token_pattern=r'(?u)\\b\\w+\\b',\n",
    "    stop_words=list(sw_es)\n",
    ")\n",
    "Xtr = tfidf_sup.fit_transform(X_train)\n",
    "Xte = tfidf_sup.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\", multi_class=\"auto\"\n",
    ")\n",
    "clf.fit(Xtr, y_train)\n",
    "y_pred = clf.predict(Xte)\n",
    "\n",
    "print(\"\\nF1 macro (test):\", round(f1_score(y_test, y_pred, average=\"macro\"), 3))\n",
    "print(\"\\nReporte de clasificación (test):\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "labels_sorted = sorted(y.unique())\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels_sorted)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"true:{c}\" for c in labels_sorted],\n",
    "                        columns=[f\"pred:{c}\" for c in labels_sorted])\n",
    "print(\"\\nMatriz de confusión:\")\n",
    "print(cm_df.to_string())\n",
    "\n",
    "# Export de test y errores\n",
    "test_df = df_sup.loc[idx_test].copy()\n",
    "test_df[\"y_true\"] = y_test.values\n",
    "test_df[\"y_pred\"] = y_pred\n",
    "test_df.to_csv(\"pred_supervisado_temas_test.csv\", index=False, encoding=\"utf-8\")\n",
    "test_df[test_df[\"y_true\"] != test_df[\"y_pred\"]].to_csv(\"errores_supervisado_temas_test.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\nExportados: pred_supervisado_temas_test.csv, errores_supervisado_temas_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b268d876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando desde https://raw.githubusercontent.com/ITALIC-US/ML-Senticon/main/senticon.es.xml...\n",
      "Lexicón guardado en 'mlsenticon_es.xml'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL del lexicón ML-SentiCon en español\n",
    "url = \"https://raw.githubusercontent.com/ITALIC-US/ML-Senticon/main/senticon.es.xml\"\n",
    "output_file = \"mlsenticon_es.xml\"\n",
    "\n",
    "print(f\"Descargando desde {url}...\")\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Lanza una excepción si hubo un error en la descarga\n",
    "\n",
    "# Guarda el contenido XML en disco\n",
    "with open(output_file, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(f\"Lexicón guardado en '{output_file}'\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f253dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs a analizar (léxico): 9036\n",
      "Lexicón (crudo) extraído: 11,542 filas\n",
      "Lexicón (normalizado): 11,342 lemas únicos\n",
      "Vocabulario léxico final: 11,342 términos\n",
      "\n",
      "Distribución etiquetas (léxico):\n",
      "lex_label\n",
      "pos    4766\n",
      "neu    2809\n",
      "neg    1461\n",
      "\n",
      "Conteos por marca (léxico) — top 15:\n",
      "                       neg   neu   pos  total\n",
      "brand_primary                                \n",
      "                      1040  1898  3023   5961\n",
      "fiduciaria previsora   145   284   466    895\n",
      "seguros bolivar         83   213   536    832\n",
      "sura                    86   178   315    579\n",
      "mapfre                  61   116   208    385\n",
      "allianz                 18    35    94    147\n",
      "liberty seguros         15    52    69    136\n",
      "hdi seguros              5    24    33     62\n",
      "axa colpatria            3     2    16     21\n",
      "porvenir                 3     5     3     11\n",
      "zurich                   2     2     3      7\n",
      "\n",
      "Proporciones por marca (léxico) — top 15:\n",
      "                        neg    neu    pos  total\n",
      "brand_primary                                   \n",
      "                      0.174  0.318  0.507    1.0\n",
      "allianz               0.122  0.238  0.639    1.0\n",
      "axa colpatria         0.143  0.095  0.762    1.0\n",
      "fiduciaria previsora  0.162  0.317  0.521    1.0\n",
      "hdi seguros           0.081  0.387  0.532    1.0\n",
      "liberty seguros       0.110  0.382  0.507    1.0\n",
      "mapfre                0.158  0.301  0.540    1.0\n",
      "porvenir              0.273  0.455  0.273    1.0\n",
      "seguros bolivar       0.100  0.256  0.644    1.0\n",
      "sura                  0.149  0.307  0.544    1.0\n",
      "zurich                0.286  0.286  0.429    1.0\n",
      "\n",
      "Exportados (léxico):\n",
      "- sentimiento_pred_mlsenticon.csv\n",
      "- sentimiento_por_marca_conteos_mlsenticon.csv\n",
      "- sentimiento_por_marca_proporciones_mlsenticon.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ML-SentiCon (ES) end-to-end para sentimiento por marca\n",
    "# ============================================\n",
    "import os, re, math, requests, xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- RUTAS & FUENTE ----------\n",
    "# Si ya tienes df_topics, úsalo; si no, usa df\n",
    "BASE = df_topics.copy() if 'df_topics' in globals() else df.copy()\n",
    "\n",
    "TEXT_COL  = \"text_clean_base\"   # puedes cambiar a \"text_raw\" si prefieres\n",
    "ID_COL    = \"id\"\n",
    "BRAND_COL = \"brand_primary\"\n",
    "\n",
    "# Ruta LOCAL preferida del XML (ajústala si la tuya es distinta)\n",
    "LEX_XML_LOCAL = r\"C:\\Users\\santi\\Downloads\\Learning\\Maestria\\Programacion Lenguaje Natural\\Proyecto\\Entrega 2\\mlsenticon_es.xml\"\n",
    "\n",
    "# URL de respaldo por si necesitas descargar el XML (raw GitHub)\n",
    "LEX_XML_URL = \"https://raw.githubusercontent.com/ITALIC-US/ML-Senticon/main/senticon.es.xml\"\n",
    "\n",
    "# ---------- 0) Preparar corpus ----------\n",
    "base_lex = BASE[[ID_COL, BRAND_COL, TEXT_COL]].copy()\n",
    "base_lex[TEXT_COL] = base_lex[TEXT_COL].fillna(\"\").astype(str)\n",
    "base_lex = base_lex[base_lex[TEXT_COL].str.strip().ne(\"\")].copy()\n",
    "print(\"Docs a analizar (léxico):\", len(base_lex))\n",
    "\n",
    "# ---------- 1) Obtener el XML de ML-SentiCon (descargar si hace falta) ----------\n",
    "LEX_XML = LEX_XML_LOCAL\n",
    "if not os.path.exists(LEX_XML_LOCAL):\n",
    "    print(f\"No se encontró el XML en:\\n  {LEX_XML_LOCAL}\\nIntento descargarlo…\")\n",
    "    try:\n",
    "        resp = requests.get(LEX_XML_URL, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        # guarda en la misma carpeta del notebook\n",
    "        LEX_XML = os.path.join(os.getcwd(), \"mlsenticon_es.xml\")\n",
    "        with open(LEX_XML, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "        print(f\"Descargado ML-SentiCon en: {LEX_XML}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"No pude descargar el lexicón desde {LEX_XML_URL}. Error: {e}\")\n",
    "# ---------- 2) Parsear XML → DataFrame (lemma, polarity) [ROBUSTO] ----------\n",
    "rows = []\n",
    "tree = ET.parse(LEX_XML)\n",
    "root = tree.getroot()\n",
    "\n",
    "def norm_tag(t):\n",
    "    return t.split('}')[-1].lower() if '}' in t else t.lower()\n",
    "\n",
    "POL_KEYS = {\"polarity\", \"priorpolarity\", \"sentiment\", \"value\", \"pol\", \"score\", \"strength\"}\n",
    "\n",
    "def extract_polarity_from_attrib(d: dict):\n",
    "    # Busca una clave de polaridad en atributos; devuelve str o None\n",
    "    for k, v in d.items():\n",
    "        if k.lower() in POL_KEYS and v is not None and str(v).strip() != \"\":\n",
    "            return str(v).strip()\n",
    "    return None\n",
    "\n",
    "for lemma in root.iter():\n",
    "    if norm_tag(lemma.tag) != \"lemma\":\n",
    "        continue\n",
    "\n",
    "    form = lemma.attrib.get(\"form\") or lemma.attrib.get(\"word\") or (lemma.text or \"\").strip()\n",
    "    if not form:\n",
    "        continue\n",
    "    form = form.strip().lower()\n",
    "\n",
    "    # 1) Caso A: la polaridad viene como atributo del propio <lemma ...>\n",
    "    pol = extract_polarity_from_attrib(lemma.attrib)\n",
    "\n",
    "    # 2) Caso B: la polaridad viene en hijos tipo <sense ... polarity=\"...\">\n",
    "    if pol is None:\n",
    "        for child in list(lemma):\n",
    "            t = norm_tag(child.tag)\n",
    "            if t in {\"sense\", \"feat\", \"polarity\"}:\n",
    "                pol = extract_polarity_from_attrib(child.attrib)\n",
    "                if pol is None:\n",
    "                    # algunos usan texto del nodo para el valor\n",
    "                    txt = (child.text or \"\").strip()\n",
    "                    if txt:\n",
    "                        pol = txt\n",
    "                if pol is not None:\n",
    "                    rows.append((form, pol))\n",
    "        # si ya agregamos por hijos, sigue al próximo lemma\n",
    "        if pol is not None and not any(r[0] == form for r in rows[-len(list(lemma)) or 0:]):\n",
    "            # se añadió vía hijos; ya continuamos\n",
    "            continue\n",
    "\n",
    "    # 3) Caso C: algún otro nieto (por seguridad)\n",
    "    if pol is None:\n",
    "        for child in lemma.iter():\n",
    "            if child is lemma:\n",
    "                continue\n",
    "            pol = extract_polarity_from_attrib(child.attrib)\n",
    "            if pol:\n",
    "                rows.append((form, pol))\n",
    "        if pol:\n",
    "            continue\n",
    "\n",
    "    # 4) Caso D: si no se encontró en hijos y sí en el lemma, añádelo\n",
    "    if pol is not None:\n",
    "        rows.append((form, pol))\n",
    "\n",
    "if not rows:\n",
    "    # Diagnóstico: imprime algunas etiquetas para ver la estructura real\n",
    "    # (No detiene; pero avisa con ValueError para que lo veas rápido)\n",
    "    raise ValueError(\n",
    "        \"No se encontraron polaridades en el XML. Es probable que el archivo tenga una estructura distinta.\\n\"\n",
    "        \"Prueba abrir el XML y buscar nodos como <sense polarity=\\\"...\\\"> o <feat name=\\\"polarity\\\" value=\\\"...\\\">.\"\n",
    "    )\n",
    "\n",
    "lex_df = pd.DataFrame(rows, columns=[\"lemma\", \"polarity\"]).dropna()\n",
    "print(f\"Lexicón (crudo) extraído: {len(lex_df):,} filas\")\n",
    "\n",
    "# Normaliza y deduplica (si un lemma tiene varias senses, promedia)\n",
    "lex_df[\"lemma\"] = lex_df[\"lemma\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "def _map_polarity(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in (\"positive\",\"pos\",\"+\",\"1\",\"true\"): return 1.0\n",
    "    if x in (\"negative\",\"neg\",\"-\",\"-1\",\"false\"): return -1.0\n",
    "    if x in (\"neutral\",\"neu\",\"0\"): return 0.0\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "lex_df[\"pol\"] = lex_df[\"polarity\"].map(_map_polarity).astype(float)\n",
    "lex_df = lex_df.groupby(\"lemma\", as_index=False)[\"pol\"].mean()\n",
    "print(f\"Lexicón (normalizado): {len(lex_df):,} lemas únicos\")\n",
    "\n",
    "\n",
    "# ---------- 3) Normalizar polaridad a float [-1, 1] y construir diccionario ----------\n",
    "def _map_polarity(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in (\"positive\",\"pos\",\"+\",\"1\",\"true\"): return 1.0\n",
    "    if x in (\"negative\",\"neg\",\"-\",\"-1\",\"false\"): return -1.0\n",
    "    if x in (\"neutral\",\"neu\",\"0\"): return 0.0\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Caso A: si aún existe 'polarity', la convertimos a 'pol'\n",
    "if \"polarity\" in lex_df.columns:\n",
    "    lex_df[\"pol\"] = lex_df[\"polarity\"].map(_map_polarity).astype(float)\n",
    "    # opcional: quitar 'polarity' para evitar confusiones posteriores\n",
    "    # lex_df = lex_df.drop(columns=[\"polarity\"])\n",
    "\n",
    "# Caso B: si ya NO existe 'polarity', asumimos que 'pol' ya está construida en el paso robusto\n",
    "if \"pol\" not in lex_df.columns:\n",
    "    raise ValueError(\"No encuentro la columna 'pol' en lex_df. Revisa el bloque de parseo robusto.\")\n",
    "\n",
    "# Asegura un único valor por lemma (si no lo hiciste ya)\n",
    "lex_df = (lex_df.groupby(\"lemma\", as_index=False)[\"pol\"]\n",
    "                 .mean())\n",
    "\n",
    "# Construye el diccionario final\n",
    "lexicon = dict(zip(lex_df[\"lemma\"], lex_df[\"pol\"]))\n",
    "print(f\"Vocabulario léxico final: {len(lexicon):,} términos\")\n",
    "\n",
    "\n",
    "# (Opcional) Ajustes de dominio: añade/ajusta términos propios del sector\n",
    "lexicon.update({\n",
    "    # señales negativas típicas del dominio\n",
    "    \"queja\": -0.6, \"reclamo\": -0.6, \"radicado\": -0.2,\n",
    "    \"siniestro\": -0.1,   # neutral/levemente negativo (ajusta a gusto)\n",
    "    \"indemnización\": 0.2, \"indemnizacion\": 0.2,\n",
    "    \"no\": 0.0,  # negador se maneja aparte (no debe tener score propio)\n",
    "    \"soat\": 0.0, \"pqrs\": -0.4\n",
    "})\n",
    "\n",
    "# ---------- 4) Tokenización + Negación + Intensificadores ----------\n",
    "TOKEN_RE = re.compile(r\"[A-Za-zÁÉÍÓÚÜÑáéíóúüñ0-9_]+\", re.UNICODE)\n",
    "\n",
    "NEGATORS = {\"no\",\"nunca\",\"jamás\",\"jamas\",\"ninguno\",\"ninguna\",\"nadie\",\"tampoco\",\"sin\"}\n",
    "INTENSIFIERS = {\n",
    "    \"muy\":1.5,\"re\":1.5,\"super\":1.5,\"súper\":1.5,\"tan\":1.2,\"bastante\":1.3,\"demasiado\":1.3,\n",
    "    \"un_poco\":0.7,\"poco\":0.7,\"algo\":0.8,\"apenas\":0.8\n",
    "}\n",
    "NEG_WINDOW = 3  # número de tokens siguientes a negar si tienen score\n",
    "\n",
    "def tokenize_simple(text: str):\n",
    "    return TOKEN_RE.findall((text or \"\").lower())\n",
    "\n",
    "def sentiment_score(tokens):\n",
    "    score = 0.0\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        tok = tokens[i]\n",
    "        mult = INTENSIFIERS.get(tok, 1.0)\n",
    "\n",
    "        # negación: invierte los próximos NEG_WINDOW con score\n",
    "        if tok in NEGATORS:\n",
    "            j = i + 1\n",
    "            flips = 0\n",
    "            while j < len(tokens) and flips < NEG_WINDOW:\n",
    "                pol = lexicon.get(tokens[j], 0.0)\n",
    "                if pol != 0.0:\n",
    "                    score += -pol\n",
    "                    flips += 1\n",
    "                j += 1\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        pol = lexicon.get(tok, 0.0)\n",
    "        if pol != 0.0:\n",
    "            score += pol * mult\n",
    "        i += 1\n",
    "    return score\n",
    "\n",
    "def label_from_score(s, pos_th=0.25, neg_th=-0.25):\n",
    "    if s >= pos_th:\n",
    "        return \"pos\"\n",
    "    if s <= neg_th:\n",
    "        return \"neg\"\n",
    "    return \"neu\"\n",
    "\n",
    "# ---------- 5) Puntuar todo el corpus ----------\n",
    "scores, labels = [], []\n",
    "for txt in base_lex[TEXT_COL].tolist():\n",
    "    toks = tokenize_simple(txt)\n",
    "    sc = sentiment_score(toks)\n",
    "    scores.append(sc)\n",
    "    labels.append(label_from_score(sc))\n",
    "\n",
    "base_lex[\"lex_score\"] = scores\n",
    "base_lex[\"lex_label\"] = labels\n",
    "\n",
    "print(\"\\nDistribución etiquetas (léxico):\")\n",
    "print(base_lex[\"lex_label\"].value_counts().to_string())\n",
    "\n",
    "# ---------- 6) Agregados por marca ----------\n",
    "agg_counts_lex = (\n",
    "    base_lex.pivot_table(index=BRAND_COL, columns=\"lex_label\", values=ID_COL, aggfunc=\"count\", fill_value=0)\n",
    "            .rename_axis(None, axis=1)\n",
    ")\n",
    "agg_counts_lex[\"total\"] = agg_counts_lex.sum(axis=1)\n",
    "agg_props_lex = agg_counts_lex.div(agg_counts_lex[\"total\"], axis=0)\n",
    "\n",
    "print(\"\\nConteos por marca (léxico) — top 15:\")\n",
    "print(agg_counts_lex.sort_values(\"total\", ascending=False).head(15).to_string())\n",
    "print(\"\\nProporciones por marca (léxico) — top 15:\")\n",
    "print(agg_props_lex.sort_values(\"total\", ascending=False).head(15).round(3).to_string())\n",
    "\n",
    "# ---------- 7) (Opcional) Comparar con pysentimiento si ya lo corriste ----------\n",
    "# Si tienes un DataFrame \"base\" de tu bloque de pysentimiento con 'sent_label'\n",
    "if 'base' in globals() and isinstance(base, pd.DataFrame) and \"sent_label\" in base.columns:\n",
    "    try:\n",
    "        comp = base[[ID_COL, BRAND_COL, TEXT_COL, \"sent_label\"]].merge(\n",
    "            base_lex[[ID_COL, \"lex_label\",\"lex_score\"]], on=ID_COL, how=\"inner\"\n",
    "        )\n",
    "        comp.to_csv(\"comparacion_lexico_vs_pysentimiento.csv\", index=False, encoding=\"utf-8\")\n",
    "        print(\"\\nExportado: comparacion_lexico_vs_pysentimiento.csv\")\n",
    "    except Exception as e:\n",
    "        print(\"Aviso: no se pudo crear comparacion_lexico_vs_pysentimiento.csv:\", e)\n",
    "\n",
    "# ---------- 8) Exports ----------\n",
    "base_lex.to_csv(\"sentimiento_pred_mlsenticon.csv\", index=False, encoding=\"utf-8\")\n",
    "agg_counts_lex.to_csv(\"sentimiento_por_marca_conteos_mlsenticon.csv\", encoding=\"utf-8\")\n",
    "agg_props_lex.to_csv(\"sentimiento_por_marca_proporciones_mlsenticon.csv\", encoding=\"utf-8\")\n",
    "print(\"\\nExportados (léxico):\")\n",
    "print(\"- sentimiento_pred_mlsenticon.csv\")\n",
    "print(\"- sentimiento_por_marca_conteos_mlsenticon.csv\")\n",
    "print(\"- sentimiento_por_marca_proporciones_mlsenticon.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "280aee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LEX] Conteos por marca (top):\n",
      "                       neg   neu   pos  total\n",
      "brand_primary                                \n",
      "                      1040  1898  3023   5961\n",
      "fiduciaria previsora   145   284   466    895\n",
      "seguros bolivar         83   213   536    832\n",
      "sura                    86   178   315    579\n",
      "mapfre                  61   116   208    385\n",
      "allianz                 18    35    94    147\n",
      "liberty seguros         15    52    69    136\n",
      "hdi seguros              5    24    33     62\n",
      "axa colpatria            3     2    16     21\n",
      "porvenir                 3     5     3     11\n",
      "zurich                   2     2     3      7\n",
      "\n",
      "[LEX] Proporciones por marca (top):\n",
      "                        neg    neu    pos  total\n",
      "brand_primary                                   \n",
      "                      0.174  0.318  0.507    1.0\n",
      "allianz               0.122  0.238  0.639    1.0\n",
      "axa colpatria         0.143  0.095  0.762    1.0\n",
      "fiduciaria previsora  0.162  0.317  0.521    1.0\n",
      "hdi seguros           0.081  0.387  0.532    1.0\n",
      "liberty seguros       0.110  0.382  0.507    1.0\n",
      "mapfre                0.158  0.301  0.540    1.0\n",
      "porvenir              0.273  0.455  0.273    1.0\n",
      "seguros bolivar       0.100  0.256  0.644    1.0\n",
      "sura                  0.149  0.307  0.544    1.0\n",
      "zurich                0.286  0.286  0.429    1.0\n",
      "\n",
      "[AVISO] No se encontró DF_PY válido con columna 'sent_label'. Saltando comparación Lex vs PySent.\n",
      "\n",
      "Archivo de ANOTACIÓN creado: informe_sentimiento\\para_anotar_sentimiento.csv\n",
      "→ Pide a un anotador que llene la columna 'gold_label' con {pos, neu, neg}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INFORME: Ejemplos, gráficas y métricas de calidad (sentimiento)\n",
    "# ============================================================\n",
    "import os, re, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_recall_fscore_support, cohen_kappa_score\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) ENTRADAS / ALIAS  (robusto)\n",
    "# ----------------------------\n",
    "# DF_LEX: debe tener [id, brand_primary, text_raw/text_clean_base, lex_label, lex_score]\n",
    "DF_LEX = base_lex.copy()\n",
    "\n",
    "# Intentamos localizar el DataFrame de pysentimiento (opcional):\n",
    "DF_PY = None\n",
    "if 'base' in globals() and isinstance(base, pd.DataFrame) and ('sent_label' in base.columns):\n",
    "    DF_PY = base.copy()\n",
    "elif 'df_pysent' in globals() and isinstance(df_pysent, pd.DataFrame) and ('sent_label' in df_pysent.columns):\n",
    "    DF_PY = df_pysent.copy()\n",
    "# si no existe, DF_PY se queda en None y se saltan las métricas de comparación\n",
    "\n",
    "# Resolver columna de texto de manera segura\n",
    "TEXT_COL_CANDIDATES = [\"text_raw\", \"text_clean_base\", \"text_topics\"]\n",
    "TEXT_COL = next((c for c in TEXT_COL_CANDIDATES if c in DF_LEX.columns), None)\n",
    "assert TEXT_COL is not None, f\"No encuentro columna de texto en DF_LEX. Probé: {TEXT_COL_CANDIDATES}\"\n",
    "\n",
    "BRAND_COL = \"brand_primary\" if \"brand_primary\" in DF_LEX.columns else \"brand\"\n",
    "ID_COL    = \"id\" if \"id\" in DF_LEX.columns else DF_LEX.columns[0]  # fallback prudente\n",
    "\n",
    "LEX_LABEL = \"lex_label\"\n",
    "LEX_SCORE = \"lex_score\"\n",
    "PY_LABEL  = \"sent_label\"  # en DF_PY\n",
    "TOPN_BRANDS = 12\n",
    "OUT_DIR = \"informe_sentimiento\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Validaciones mínimas\n",
    "for col in [ID_COL, BRAND_COL, TEXT_COL, LEX_LABEL, LEX_SCORE]:\n",
    "    assert col in DF_LEX.columns, f\"Falta columna {col} en DF_LEX\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) AGREGADOS BÁSICOS\n",
    "# ----------------------------\n",
    "def agregados_por_marca(df, label_col):\n",
    "    counts = (df\n",
    "              .pivot_table(index=BRAND_COL, columns=label_col, values=ID_COL, aggfunc=\"count\", fill_value=0)\n",
    "              .rename_axis(None, axis=1))\n",
    "    counts[\"total\"] = counts.sum(axis=1)\n",
    "    props = counts.div(counts[\"total\"], axis=0)\n",
    "    return counts, props\n",
    "\n",
    "lex_counts, lex_props = agregados_por_marca(DF_LEX, LEX_LABEL)\n",
    "lex_counts.to_csv(os.path.join(OUT_DIR, \"conteos_por_marca_lex.csv\"), encoding=\"utf-8\")\n",
    "lex_props.to_csv(os.path.join(OUT_DIR, \"proporciones_por_marca_lex.csv\"), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n[LEX] Conteos por marca (top):\")\n",
    "print(lex_counts.sort_values(\"total\", ascending=False).head(15).to_string())\n",
    "print(\"\\n[LEX] Proporciones por marca (top):\")\n",
    "print(lex_props.sort_values(\"total\", ascending=False).head(15).round(3).to_string())\n",
    "\n",
    "# ----------------------------\n",
    "# 2) EJEMPLOS (muestras)\n",
    "# ----------------------------\n",
    "def ejemplos(df, brand=None, label=None, n=10):\n",
    "    tmp = df.copy()\n",
    "    if brand is not None:\n",
    "        tmp = tmp[tmp[BRAND_COL].astype(str).str.lower() == str(brand).lower()]\n",
    "    if label is not None:\n",
    "        tmp = tmp[tmp[LEX_LABEL] == label]\n",
    "    n = min(n, len(tmp))\n",
    "    if n == 0:\n",
    "        return pd.DataFrame(columns=[ID_COL, BRAND_COL, LEX_LABEL, LEX_SCORE, TEXT_COL])\n",
    "    asc = (label == \"neg\") if label is not None else False\n",
    "    return (tmp[[ID_COL, BRAND_COL, LEX_LABEL, LEX_SCORE, TEXT_COL]]\n",
    "            .sample(n, random_state=42)\n",
    "            .sort_values(LEX_SCORE, ascending=asc))\n",
    "\n",
    "\n",
    "# Exporta ejemplos por clase (global) y por top marcas\n",
    "ej_pos = ejemplos(DF_LEX, label=\"pos\", n=30)\n",
    "ej_neu = ejemplos(DF_LEX, label=\"neu\", n=30)\n",
    "ej_neg = ejemplos(DF_LEX, label=\"neg\", n=30)\n",
    "ej_pos.to_csv(os.path.join(OUT_DIR, \"ejemplos_pos.csv\"), index=False, encoding=\"utf-8\")\n",
    "ej_neu.to_csv(os.path.join(OUT_DIR, \"ejemplos_neu.csv\"), index=False, encoding=\"utf-8\")\n",
    "ej_neg.to_csv(os.path.join(OUT_DIR, \"ejemplos_neg.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "top_brands = lex_counts.sort_values(\"total\", ascending=False).head(TOPN_BRANDS).index.tolist()\n",
    "with pd.ExcelWriter(os.path.join(OUT_DIR, \"ejemplos_por_marca.xlsx\"), engine=\"xlsxwriter\") as wr:\n",
    "    for b in top_brands:\n",
    "        sub = pd.concat([\n",
    "            ejemplos(DF_LEX, brand=b, label=\"neg\", n=15),\n",
    "            ejemplos(DF_LEX, brand=b, label=\"neu\", n=10),\n",
    "            ejemplos(DF_LEX, brand=b, label=\"pos\", n=10)\n",
    "        ], axis=0)\n",
    "        sub.to_excel(wr, sheet_name=(b[:30] or \"sinmarca\"), index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) GRÁFICAS (matplotlib, sin seaborn)\n",
    "# ----------------------------\n",
    "# Proporción de sentimiento por marca (Top) — apilada\n",
    "plt.figure(figsize=(12, 5))\n",
    "lex_props_sorted = lex_props.sort_values(\"total\", ascending=False).head(TOPN_BRANDS)\n",
    "order = [c for c in [\"neg\",\"neu\",\"pos\"] if c in lex_props_sorted.columns]\n",
    "bottom = np.zeros(len(lex_props_sorted))\n",
    "for cls in order:\n",
    "    vals = lex_props_sorted[cls].values\n",
    "    plt.bar(lex_props_sorted.index.astype(str), vals, bottom=bottom, label=cls)\n",
    "    bottom = bottom + vals\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Proporción de sentimiento por marca (Top marcas) — Lexicón\")\n",
    "plt.ylabel(\"Proporción\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"stacked_proporcion_sentimiento_por_marca_lex.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "lex_counts_sorted = lex_counts.sort_values(\"total\", ascending=False).head(TOPN_BRANDS)\n",
    "plt.bar(lex_counts_sorted.index.astype(str), lex_counts_sorted[\"total\"].values)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Volumen por marca (Top marcas)\")\n",
    "plt.ylabel(\"Docs\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"barras_volumen_por_marca.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# Distribución global\n",
    "plt.figure(figsize=(6,4))\n",
    "dist_global = DF_LEX[LEX_LABEL].value_counts()\n",
    "plt.bar(dist_global.index.astype(str), dist_global.values)\n",
    "plt.title(\"Distribución global de sentimiento (Lexicón)\")\n",
    "plt.ylabel(\"Docs\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"distribucion_global_lex.png\"), dpi=160)\n",
    "plt.close()\n",
    "# ----------------------------\n",
    "# 4) MÉTRICAS DE CALIDAD (sin gold): acuerdo Lex vs Pysentimiento\n",
    "# ----------------------------\n",
    "if isinstance(DF_PY, pd.DataFrame) and (PY_LABEL in DF_PY.columns):\n",
    "    cmp_df = (DF_LEX[[ID_COL, LEX_LABEL]]\n",
    "              .merge(DF_PY[[ID_COL, PY_LABEL]], on=ID_COL, how=\"inner\")\n",
    "              .dropna(subset=[LEX_LABEL, PY_LABEL]))\n",
    "    print(\"\\nTamaño de intersección Lex vs PySent:\", len(cmp_df))\n",
    "\n",
    "    labels = sorted(list(set(cmp_df[LEX_LABEL].unique()) | set(cmp_df[PY_LABEL].unique())))\n",
    "    cm = confusion_matrix(cmp_df[PY_LABEL], cmp_df[LEX_LABEL], labels=labels)\n",
    "    print(\"\\nMatriz de confusión (Pysent → filas, Lex → columnas):\")\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"py:{l}\" for l in labels], columns=[f\"lex:{l}\" for l in labels])\n",
    "    print(cm_df.to_string())\n",
    "\n",
    "    print(\"\\nReporte vs. pysentimiento (macro):\")\n",
    "    print(classification_report(cmp_df[PY_LABEL], cmp_df[LEX_LABEL], labels=labels, digits=3))\n",
    "\n",
    "    kappa = cohen_kappa_score(cmp_df[PY_LABEL], cmp_df[LEX_LABEL])\n",
    "    print(\"\\nCohen's kappa (Lex vs PySent):\", round(kappa, 3))\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(\"Confusión PySent vs Lex\")\n",
    "    plt.xticks(range(len(labels)), labels)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.xlabel(\"Lex (pred)\")\n",
    "    plt.ylabel(\"PySent (ref)\")\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"confusion_pysent_vs_lex.png\"), dpi=160)\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"\\n[AVISO] No se encontró DF_PY válido con columna 'sent_label'. Saltando comparación Lex vs PySent.\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) DRIVERS / TÉRMINOS CARACTERÍSTICOS\n",
    "#    (n-gramas más frecuentes por clase y por marca)\n",
    "# ----------------------------\n",
    "TOKEN_RE = re.compile(r\"[A-Za-zÁÉÍÓÚÜÑáéíóúüñ0-9]+\", re.UNICODE)\n",
    "def ngrams(tokens, n=2):\n",
    "    return [\"_\".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def top_terms(df, label=None, brand=None, n_top=25, ngram=(1,2)):\n",
    "    tmp = df.copy()\n",
    "    if label is not None:\n",
    "        tmp = tmp[tmp[LEX_LABEL] == label]\n",
    "    if brand is not None:\n",
    "        tmp = tmp[tmp[BRAND_COL] == brand]\n",
    "    bag = []\n",
    "    for txt in tmp[TEXT_COL].astype(str):\n",
    "        toks = [t.lower() for t in TOKEN_RE.findall(txt)]\n",
    "        if 1 in ngram:\n",
    "            bag.extend(toks)\n",
    "        if 2 in ngram:\n",
    "            bag.extend(ngrams(toks, 2))\n",
    "    return Counter(bag).most_common(n_top)\n",
    "\n",
    "# Global drivers por clase\n",
    "drivers = {}\n",
    "for lab in [\"neg\",\"neu\",\"pos\"]:\n",
    "    drivers[lab] = top_terms(DF_LEX, label=lab, n_top=40, ngram=(1,2))\n",
    "    pd.DataFrame(drivers[lab], columns=[\"term\",\"freq\"]).to_csv(\n",
    "        os.path.join(OUT_DIR, f\"drivers_{lab}_global.csv\"), index=False, encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# Drivers por marca (solo top marcas)\n",
    "for b in top_brands:\n",
    "    for lab in [\"neg\",\"pos\"]:\n",
    "        dd = top_terms(DF_LEX, label=lab, brand=b, n_top=30, ngram=(1,2))\n",
    "        pd.DataFrame(dd, columns=[\"term\",\"freq\"]).to_csv(\n",
    "            os.path.join(OUT_DIR, f\"drivers_{lab}_{b[:30]}.csv\"), index=False, encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "# ----------------------------\n",
    "# 6) ANOTACIÓN HUMANA (estratificada) + EVALUACIÓN\n",
    "# ----------------------------\n",
    "def sample_for_annotation(df, per_label=60, per_brand=0):\n",
    "    # Estratifica por sentimiento y, opcionalmente, por marca\n",
    "    frames = []\n",
    "    labels = df[LEX_LABEL].dropna().unique().tolist()\n",
    "    if per_brand > 0:\n",
    "        brands = df[BRAND_COL].value_counts().index.tolist()\n",
    "        for b in brands[:TOPN_BRANDS]:\n",
    "            for lab in labels:\n",
    "                subset = df[(df[BRAND_COL]==b) & (df[LEX_LABEL]==lab)]\n",
    "                k = min(per_brand, len(subset))\n",
    "                if k>0:\n",
    "                    frames.append(subset.sample(k, random_state=42))\n",
    "    else:\n",
    "        for lab in labels:\n",
    "            subset = df[df[LEX_LABEL]==lab]\n",
    "            k = min(per_label, len(subset))\n",
    "            if k>0:\n",
    "                frames.append(subset.sample(k, random_state=42))\n",
    "    if frames:\n",
    "        return pd.concat(frames, axis=0).drop_duplicates(subset=[ID_COL])\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "ann = sample_for_annotation(DF_LEX, per_label=80, per_brand=0)\n",
    "ann = ann[[ID_COL, BRAND_COL, TEXT_COL, LEX_LABEL, LEX_SCORE]].copy()\n",
    "ann[\"gold_label\"] = \"\"  # para que un humano complete {pos,neu,neg}\n",
    "ann_path = os.path.join(OUT_DIR, \"para_anotar_sentimiento.csv\")\n",
    "ann.to_csv(ann_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nArchivo de ANOTACIÓN creado: {ann_path}\")\n",
    "print(\"→ Pide a un anotador que llene la columna 'gold_label' con {pos, neu, neg}\")\n",
    "\n",
    "# Si ya tienes anotado un archivo gold, carga y evalúa\n",
    "GOLD_IN = os.path.join(OUT_DIR, \"para_anotar_sentimiento_gold.csv\")  # renombra a este cuando lo tengas completo\n",
    "if os.path.exists(GOLD_IN):\n",
    "    gold = pd.read_csv(GOLD_IN)\n",
    "    gold = gold.dropna(subset=[\"gold_label\"])\n",
    "    # une con tus predicciones actuales (lex)\n",
    "    eval_df = gold.merge(DF_LEX[[ID_COL, LEX_LABEL]], on=ID_COL, how=\"left\", suffixes=(\"_gold\",\"_lex\"))\n",
    "    eval_df = eval_df.dropna(subset=[LEX_LABEL])\n",
    "    print(\"\\nEvaluación vs. GOLD:\")\n",
    "    print(classification_report(eval_df[\"gold_label\"], eval_df[LEX_LABEL], digits=3))\n",
    "    # Matriz y kappa\n",
    "    labs_eval = [\"neg\",\"neu\",\"pos\"]\n",
    "    cm_gold = confusion_matrix(eval_df[\"gold_label\"], eval_df[LEX_LABEL], labels=labs_eval)\n",
    "    print(\"\\nMatriz de confusión (GOLD vs LEX):\")\n",
    "    print(pd.DataFrame(cm_gold, index=[f\"gold:{l}\" for l in labs_eval], columns=[f\"lex:{l}\" for l in labs_eval]).to_string())\n",
    "    print(\"Kappa (GOLD vs LEX):\", round(cohen_kappa_score(eval_df[\"gold_label\"], eval_df[LEX_LABEL]), 3))\n",
    "    # Grabar reporte\n",
    "    with open(os.path.join(OUT_DIR, \"reporte_gold_vs_lex.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(classification_report(eval_df[\"gold_label\"], eval_df[LEX_LABEL], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991cc3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset final construido: 15469 registros, 23 columnas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Corpus base con métricas léxicas y marcas\n",
    "df = pd.read_csv(\"corpus_medido.csv\")\n",
    "\n",
    "# Tópicos globales (KMeans general)\n",
    "df_topics = pd.read_csv(\"corpus_con_temas_kmeans.csv\")\n",
    "\n",
    "# Tópicos de servicio\n",
    "df_service = pd.read_csv(\"corpus_servicio_temas_kmeans.csv\")\n",
    "\n",
    "# Sentimiento (ML-SentiCon)\n",
    "base_lex = pd.read_csv(\"sentimiento_pred_mlsenticon.csv\")\n",
    "\n",
    "def build_final_dataset(df_base, df_topics=None, df_service=None, df_sent=None):\n",
    "    import pandas as pd\n",
    "    df_final = df_base.copy()\n",
    "    if df_topics is not None and all(c in df_topics.columns for c in [\"topic_km\", \"topic_name\"]):\n",
    "        df_final.loc[df_topics.index, \"topic_km_global\"] = df_topics[\"topic_km\"]\n",
    "        df_final.loc[df_topics.index, \"topic_name_global\"] = df_topics[\"topic_name\"]\n",
    "    if df_service is not None and all(c in df_service.columns for c in [\"topic_km\", \"topic_name\"]):\n",
    "        df_final.loc[df_service.index, \"topic_km_service\"] = df_service[\"topic_km\"]\n",
    "        df_final.loc[df_service.index, \"topic_name_service\"] = df_service[\"topic_name\"]\n",
    "    if df_sent is not None and all(c in df_sent.columns for c in [\"lex_label\", \"lex_score\"]):\n",
    "        df_final = df_final.merge(df_sent[[\"id\", \"lex_label\", \"lex_score\"]], on=\"id\", how=\"left\")\n",
    "    cols_order = [\n",
    "        \"id\", \"Date\", \"brand_primary\", \"brand_list\", \"text_raw\", \"text_clean_base\",\n",
    "        \"flag_corp_news\", \"flag_cm_reply\", \"n_chars_raw\", \"n_tokens_lemma\",\n",
    "        \"n_tokens_topics\", \"ttr_lemma\", \"hapax_lemma\",\n",
    "        \"topic_km_global\", \"topic_name_global\",\n",
    "        \"topic_km_service\", \"topic_name_service\",\n",
    "        \"lex_label\", \"lex_score\"\n",
    "    ]\n",
    "    cols_order = [c for c in cols_order if c in df_final.columns]\n",
    "    df_final = df_final[cols_order + [c for c in df_final.columns if c not in cols_order]]\n",
    "    print(f\"Dataset final construido: {df_final.shape[0]} registros, {df_final.shape[1]} columnas\")\n",
    "    return df_final\n",
    "\n",
    "df_final = build_final_dataset(\n",
    "    df_base=df,\n",
    "    df_topics=df_topics,\n",
    "    df_service=df_service,\n",
    "    df_sent=base_lex\n",
    ")\n",
    "\n",
    "df_final.to_csv(\n",
    "    \"dataset_final_completo.csv\",\n",
    "    sep=\";\",               # separador punto y coma\n",
    "    index=False,           # no incluir el índice\n",
    "    encoding=\"utf-8\",      # codificación estándar\n",
    "    quoting=1,             # 1 = csv.QUOTE_ALL (cita todas las celdas)\n",
    "    quotechar='\"',         # usa comillas dobles para encerrar texto\n",
    "    lineterminator=\"\\n\"    # ✅ nombre correcto del argumento\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997a6a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Date', 'brand_primary', 'brand_list', 'text_raw',\n",
       "       'text_clean_base', 'n_chars_raw', 'n_tokens_lemma', 'n_tokens_topics',\n",
       "       'ttr_lemma', 'hapax_lemma', 'topic_km_global', 'topic_name_global',\n",
       "       'topic_km_service', 'topic_name_service', 'lex_label', 'lex_score',\n",
       "       'Domain', 'Url', 'text_blocked', 'text_lemma', 'text_stem',\n",
       "       'text_topics'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accb6c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand_primary\n",
       "sura                  1264\n",
       "seguros bolivar       1036\n",
       "mapfre                 605\n",
       "allianz                310\n",
       "axa colpatria          237\n",
       "liberty seguros        143\n",
       "hdi seguros             90\n",
       "seguros del estado      34\n",
       "porvenir                25\n",
       "zurich                   7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['brand_primary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff11c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
